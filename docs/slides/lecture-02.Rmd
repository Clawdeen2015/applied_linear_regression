---
title: "Inferences in Simple Linear Regression"
subtitle: "Lecture 02"
author: "Brandon M. Greenwell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: [default, custom.css, hygge]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: clear, center, middle

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)

# Global chunk options
knitr::opts_chunk$set(
  echo = TRUE,
  dev = "png",
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "70%",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

# Load required packages
library(ggplot2)
```

.font300[

[R code for these slides](https://github.com/bgreenwell/uc-bana7052/blob/master/code/lecture-02.R)

]


---
class: clear, middle, center

background-image: url(images/research-walberg-normal-distribution.jpg)


---

# Reading assignment

.font200[

* Chapter: 2

    - Sections: 2.1-2.11

* Main topics: 

    - Statistical inferences for simple linear regression
  
]


---

# Prerquisites

.scrollable[

```{r prerequisites, eval=FALSE}
# List of required (CRAN) packages
pkgs <- c(
  "ggplot2",    # for awesome graphics
  "investr",    # for data sets and plotFit() functions
)

# Install required (CRAN) packages
for (pkg in pkgs) {
  if (!(pkg %in% installed.packages()[, "Package"])) {
    install.packages(pkg)
  }
}
```

]


---
class: clear 

background-image: url(images/significance.png)
background-size: 40%


---
class: clear, middle

```{r relationship-01, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
set.seed(101)
x <- rep(1:5, each = 10)
y <- 1 + 1*x + rnorm(length(x), sd = 3)
ggplot(data.frame(x, y), aes(x, y)) +
  geom_point() +
  labs(x = "X", y = "Y")
```


---
class: clear, middle

```{r relationship-02, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
set.seed(101)
x <- rep(1:5, each = 10)
y <- 1 + 1*x + rnorm(length(x), sd = 3)
ggplot(data.frame(x, y), aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, col = "red2") +
  labs(x = "X", y = "Y")
```


---
class: clear, middle

```{r relationship-03, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
set.seed(101)
x <- rep(1:5, each = 10)
y <- 1 + 1*x + rnorm(length(x), sd = 3)
ggplot(data.frame(x, y), aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = TRUE, col = "red2") +
  labs(x = "X", y = "Y")
```


---

# Inferences concerning $\beta_1$

.font150[

* **Bad:** Is there a relationship between $X$ and $Y$? (.red[not testable])

]

--

.font150[

* **Good:** Is there a statistically significant linear relationship between $X$ and $Y$ at the $\alpha = 0.05$ level? (.green[testable])

]

--

.font150[

* How can we reformulate this as a statistical test?

]

--

.font150[

$$
H_0: \beta_1 = 0 \quad vs \quad H_1: \beta_1 \ne 0
$$

]

--

.center.font125.content-box-yellow[

Need a point estimate, test statistic, and reference distribution!

]


---

# Properties of $\widehat{\beta}_1$

<br><br><br>

.font250.center[

What does the *bias* of a parameter estimate refer to? `r emo::ji("thinking")`

]


---
class: clear

.code150[

```{r simulation-01}
# Simulate data
n <- 100
set.seed(8451)
x <- runif(n, min = 0, max = 10)
y <- rnorm(n, mean = 1 + 10*x, sd = 10)  #<<

# Fit an SLR model
fit <- lm(y ~ x)
coef(fit)
```

]


---
class: clear

.code150[

```{r simulation-02, fig.width=6, fig.asp=0.618, out.width="70%"}
# Plot the results
plot(x, y)
abline(fit, lwd = 2, col = "red2")
```

]


---
class: clear

```{r simulation-03, cache=TRUE}
# Run simulation
set.seed(8451)
sim <- t(replicate(10000, expr = {
  x <- runif(n = 100, min = 0, max = 10)
  y <- rnorm(n = 100, mean = 1 + 10*x, sd = 10)
  coef(lm(y ~ x))
}))

# Sample means
apply(sim, MARGIN = 2, FUN = mean)

# Sample standard deviations
apply(sim, MARGIN = 2, FUN = sd)
```


---
class: clear

```{r simulation-04, fig.width=6, fig.asp=0.618, out.width="80%"}
# Sampling distribution
hist(sim[, 2], breaks = 50, freq = FALSE, col = "gray30", border = "white", 
     las = 1, xlab = expression(hat(beta)[1]),
     main = expression(paste("Sampling distribution of ", hat(beta)[1])))
abline(v = 10, lwd = 3, col = "red2")
```


---

# Properties of $\widehat{\beta}_1$

.font150[

* Recall from the previous lecture that LS estimation provides the best linear unbiased estimates .blue[(BLUE)] of $\beta_0$ and $\beta_1$; namely, $\widehat{\beta}_0$ and $\widehat{\beta}_1$

    - Unbiased since $E\left[\widehat{\beta}_0\right] = \beta_0$ and $E\left[\widehat{\beta}_1\right] = \beta_1$

    - Best in the sense that $\widehat{\beta}_0$ and $\widehat{\beta}_1$ have the **smallest variance** among all other **linear unbiased** estimators of $\beta_0$ and $\beta_1$, respectively

]

--

.font125.center.red[

So what is $Var\left[\widehat{\beta}_0\right]$ and $Var\left[\widehat{\beta}_1\right]$?

]


---

# Properties of $\widehat{\beta}_1$

.font150[

* Recall that the LS estimate of the slope is a weighted average of the (observed) response values: $\widehat{\beta}_1 = \sum_{i=1}^n w_iY_i$ 

]

--

.font150[

* Since the $Y_i$ are independent, it follows that 

.font75[

$$Var\left(\widehat{\beta}_1\right) = Var\left(\sum_{i=1}^n w_iY_i\right) = \sum_{i=1}^n w_i^2Var\left(Y_i\right) = \dots = \sigma^2 / S_{xx}$$

]

]


---

# Sampling distribution of $\widehat{\beta}_1$

.font150[

* Assuming $\epsilon_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)$, then $\widehat{\beta}_1 \sim ???$ `r emo::ji("thinking")`

]

--
 
.font150[

* $\widehat{\beta}_1 \sim N\left(\beta_1, \sigma^2/S_{xx}\right)$

]

--

.font150[

* But we generally don't know $\sigma^2$, so how do we estimate it?

]

.font150[

* Replace $\sigma^2$ with its point estimate $\widehat{\sigma}^2 = MSE$, and the sampling distribution of $\widehat{\beta}_1$ becomes $\widehat{\beta}_1 \sim t_{n-2}$

]

--

.font150.center[

$\widehat{\beta}_1 \sim N\left(\beta_1, \widehat{\sigma}^2/S_{xx}\right)$ as $n \rightarrow \infty$

]


---

# Standard errors

.font150[

* .purple[The standard deviation of an estimate is referred to as its *standard error*]. For example,

$$\sqrt{Var\left(\widehat{\beta}_1\right)} = SE\left(\widehat{\beta}_1\right) = \sigma/\sqrt{S_{xx}}$$

]

--

.font150[

* Since we don't know $\sigma^2$, we estimate $SE\left(\widehat{\beta}_1\right) = \sigma/\sqrt{S_{xx}}$ with its *plug-in* estimate

$$\widehat{SE}\left(\widehat{\beta}_1\right) = \widehat{\sigma}/\sqrt{S_{xx}}$$

]


---

# Inference regarding $\beta_1$

* Hypothesis test: $H_0: \beta_1 = c \quad vs \quad H_1: \beta_1 \ne c$

--

* Test statistic: $$t_{obs} = \frac{\widehat{\beta}_1 - c}{\widehat{SE}\left(\widehat{\beta}_1\right)} = \frac{\widehat{\beta}_1 - c}{\widehat{\sigma} / \sqrt{S_{xx}}}$$

--

* Rejection $H_0$ whenever $\left|t_{obs}\right| \ge t_{n - 2, 1 - \alpha/2}$

    - In R, $t$ quantiles can be obtained using `qt(1 - alpha/2, df = n-2)`, for example
    
```{r qt}
alpha <- 0.05           # significance level
n <- 30                 # sample size         
qt(1 - alpha/2, n - 2)  # cutoff value        #<<
```


---

# Inference regarding $\beta_1$

<br><br>

.font250.center[ 

A $\left(1-\alpha\right)$ 100% confidence interval for $\beta_1$ is given by $$\widehat{\beta}_1 \pm t_{n - 2, 1 - \alpha/2}\widehat{\sigma}/S_{xx}$$

]


---

# Rocket propellant example

```{r rocket-load}
# Load the rocket propellant data
url <- "https://bgreenwell.github.io/uc-bana7052/data/rocket.csv"
rocket <- read.csv(url)

# Print first six rows
tibble::as_tibble(rocket)[1:6, ]
```


---

# Rocket propellant example

.code125[

```{r rocket-slr, eval=FALSE}
# Load required packages
library(investr)

# Fit an SLR model
rocket_fit <- lm(strength ~ age, data = rocket)

# Plot the data with the fitted mean response
plotFit(rocket_fit, lwd.fit = 2, 
        col.fit = "red2", pch = 19)
```

]


---

# Rocket propellant example

```{r rocket-slr-plot, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# Load required packages
library(investr)

# Fit an SLR model
rocket_fit <- lm(strength ~ age, data = rocket)

# Plot the data with the fitted mean response
plotFit(rocket_fit, lwd.fit = 2, col.fit = "red2", pch = 19)
```


---

# Rocket propellant example

.scrollable[

```{r rocket-slr-summary}
# Print a summary of the fitted model
summary(rocket_fit)
```

]


---

# Rocket propellant example

.code150[

```{r rocket-slr-confint}
# Compute a 95% CI for the slope
confint(rocket_fit, level = 0.95)  #<<
```

]


---

# Rocket propellant example

.font150[

Can you interpret the confidence interval for $\beta_1$ in the previous example?

]

.code125[

```{r rocket-slr-confint-02}
confint(rocket_fit, level = 0.95)
```

]

--

.font150[

With 95% confidence, we estimate that the mean strength of rockets .bold.red[decreases] between 31.08 psi and 43.22 psi for every one-week increase in age.

]


---

# Your turn

.font250[

Fit an SLR model to the crystal weight data using `weight` as the response and `time` as the predictor. Find a 95% confidence interval for the slope and interpret the results in **plain english**.

]


---

# Solution

.code130[

```{r crystal-slr, eval=FALSE}
# Load the crystal weight data
data(crystal, package = "investr")

# Fit an SLR model
crystal_fit <- lm(weight ~ time, data = crystal)

# Plot the data with the fitted mean response
plotFit(crystal_fit, lwd.fit = 2, 
        col.fit = "red2", pch = 19)
```

]


---

# Solution

```{r crystal-slr-plot, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# Load the crystal weight data
data(crystal, package = "investr")

# Fit an SLR model
crystal_fit <- lm(weight ~ time, data = crystal)

# Plot the data with the fitted mean response
plotFit(crystal_fit, lwd.fit = 2, 
        col.fit = "red2", pch = 19)
```


---

# Solution

.scrollable[

```{r crystal-slr-summary}
# Print a summary of the model
summary(crystal_fit)
```

]


---

# Solution

.code150[

```{r crystal-slr-confint}
# Compute 95% CIs
confint(crystal_fit, level = 0.95)  #<<
```

]

.font150[

With 95% confidence, we estimate that the average weight of crystals increases between 0.43 grams and 0.58 grams for every one-hour increase in growth time.

]


---
class: clear, middle

.font250[

Using the rocket propellant example, test whether the slope significantly differs from $-40$ psi/week at the $\alpha = 0.05$ level.

]


---
class: clear

.code125[

```{r rocket-slope-test-01}
# Extract summary of estimated slope
(slope <- summary(rocket_fit)$coef["age", 1:2])  #<<

# Compute test statistic
(t_obs <- (slope["Estimate"] + 40) / 
    slope["Std. Error"])
```

]


---
class: clear

.code125[

```{r rocket-slope-test-02}
# Compute cutoff from reference distribution
alpha <- 0.05
n <- nrow(rocket)
(t_ref <- qt(1 - alpha/2, df = n - 2))  #<<

# Decision rule
abs(t_obs) > t_ref
```

]


---

# Your turn

.font250[

Using the crystal weight example, test whether the slope significantly differs from $3/4$ grams/hour at the $\alpha = 0.1$ level.

]


---

# Solution

.font150[

$H_0: \beta_1 = 3/4 \quad vs \quad H_1: \beta_1 \ne 3/4$

```{r 01-crystal-solution-02}
# Compute a 90% CI for the slope
confint(crystal_fit, parm = "time", level = 0.9)
```

Since 3/4 lies outside of the 90% confidence interval for $\beta_1$, we reject the null hypothesis at the 0.1 level and conclude that the slope significantly differs from 3/4. 

]


---

# Computing the *p*-value

.font150[

One-sided test: $p = Pr\left(T_{n-2} > \left|t_{obs}\right|\right)$

]

.code125[

```{r p-value-01}
# Rocket propellant example
(t_obs <- slope["Estimate"] / slope["Std. Error"])
pt(abs(t_obs), df = nrow(rocket) - 2, lower.tail = FALSE)
```

]


---

# Computing the *p*-value

.font150[

Two-sided test: $p = 2 \times Pr\left(T_{n-2} > \left|t_{obs}\right|\right)$

]

.code125[

```{r p-value-02}
# Rocket propellant example
(t_obs <- slope["Estimate"] / slope["Std. Error"])
2 * pt(abs(t_obs), df = nrow(rocket) - 2, lower.tail = FALSE)
```

]


---

# Your turn

.font300[

Compute the *p*-value for the previous test in the crystal weight example. What is your decision?

]


---

# Solution

.font125[

$H_0: \beta_1 = 3/4 \quad vs \quad H_1: \beta_1 \ne 3/4$

```{r 01-crystal-solution-03}
slope <- summary(crystal_fit)$coef["time", ]
(t_obs <- (slope["Estimate"] - 3/4) / slope["Std. Error"])  #<<
(p_val <- 2 * pt(abs(t_obs), df = nrow(crystal) - 2, lower.tail = FALSE))  #<<
```

Since $p < 0.1$, we reject the null hypothesis and conclude that the slope significantly differs from 3/4. 

]


---
class: clear, middle

.font200[

Similar results exist for the intercept, just replace $\widehat{SE}\left(\widehat{\beta}_1\right)$ with $$\widehat{SE}\left(\widehat{\beta}_0\right) = MSE\left(\frac{1}{n} + \frac{\bar{X}^2}{S_{xx}}\right)$$

]


---
class: clear

.font200[

Consider the following hypotheses for the SLR model: $$H_0: \beta_1 = 0 \quad vs \quad H_1: \beta_1 \ne 0$$

]

--

.font200.center.red[

What does failing to reject $H_0$ imply about the relationship between $X$ and $Y$?

]


---

# ANOVA approach

.font150[

* What does ANOVA refer to? `r emo::ji("thinking")`

]

--

.font150[

* Partitioning sums of squares (SS)

    - Total SS: $SST = SS_{tot} = \sum_{i=1}^n\left(Y_i - \bar{Y}\right)^2$
    
    - Error SS: $SSE = SS_{err} = \sum_{i=1}^n\left(Y_i - \widehat{Y}_i\right)^2$
    
    - Regression SS: $SSR = SS_{reg} = \sum_{i=1}^n\left(\widehat{Y}_i - \bar{Y}\right)^2$
    
]


---

# ANOVA approach

.font150[

* $\left(Y_i - \bar{Y}\right) = \left(\widehat{Y}_i - \bar{Y}\right) + \left(Y_i - \widehat{Y}_i\right)$

]

-- 

.font150[

* It is easy to show that the sums of these squared deviations have the same relationship: $$\sum_{i=1}^n\left(Y_i - \bar{Y}\right) = \sum_{i=1}^n\left(\widehat{Y}_i - \bar{Y}\right) + \sum_{i=1}^n\left(Y_i - \widehat{Y}_i\right)$$

    - In other words, $SST = SSS + SSE$ (**much like in a one-way ANOVA**)

]


---

# ANOVA approach

.font200[

* Diving an SS by its associated *degrees of freedom* (df) produces mean squares (.purple[kind of like a standard deviation])

]

--

.font200[

* $MSR = \frac{SSR}{1}$

* $MSE = \frac{SSE}{1}$

]


---
class: clear, middle, center

.font250[

Is there a (linear) relationship between $X$ and $Y$? $$H_0: \beta_1 = 0 \quad vs \quad H_1: \beta_1 \ne 0$$

]


---

# ANOVA approach

.font150[

* Test statistic: $$F_{obs} = \frac{MSR}{MSE}$$ with 1 numerator DF and $n-2$ denominator DF

]

--

.font150[

* Reject $H_0$ at the $\alpha$ level whenever $F_{obs} > F_{1-\alpha, 1, n-2}$

    - Notice the use of $\alpha$ as opposed to $\alpha/2$ `r emo::ji("thinking")`
    
    - Is a large value of $F_{obs}$ good or bad?

]


---

# Rocket propellant example

.scrollable[

```{r rocket-anova}
# Compute ANOVA table for the fitted model
anova(rocket_fit)  #<<

# Print summary of fitted model
summary(rocket_fit)
```

]


---

# Your turn

.font200[

Using the crystal weight example, use an *F*-test to test whether or not there is a relationship between `time` and `weight` at the $\alpha = 0.05$ level. Manually compute the *p*-value for this test and `r emo::ji("pray")` that it matches the output from `summary()`.

]


---

# Solution

.scrollable[

```{r 01-crystal-solution-04}
# Print summary of the fitted model
summary(crystal_fit)

# What values can we pull out from summary()
names(summary(crystal_fit))

# Observed test statitic
f_obs <- summary(crystal_fit)$fstatistic

# Compute p-value (one approach)
pf(f_obs, df1 = 1, df2 = nrow(crystal) - 2, lower.tail = FALSE)

# Compute p-value (another approach)
1 - pf(f_obs, df1 = 1, df2 = nrow(crystal) - 2)
```

]


---

# F or t?

.font150[

* For every two-sided *t*-test, there is a corresponding *F*-test

    - $t_{obs}^2 = F_{obs}$

]

--

.font150[

* In the SLR model, these the approaches are equivalent 

    - The *F*-test becomes useful when we start adding more predictors (i.e., in multiple linear regression)

]


---

# The general linear test

.font150[

* Full model: $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$

]

--

.font150[

* Reduced model: $Y_i = \beta_0 + \epsilon_i$

]

--

.font150[

* Implied test: $H_0: \beta_1 = 0 \quad vs \quad H_1: \beta_1 \ne 0$

    - $F_{obs} = \frac{SSE(R) - SSE(F)}{df_R - df_F} \div \frac{SSE(F)}{df_F}$
    
    - Reject $H_0$ whenever $F_{obs} > F_{1 - \alpha, df_R - df_F, df_F}$
    
    - More useful in multiple linear regression, but we'll introduce it here!

]


---

# Rocket propellant example

.scrollable[

```{r rocket-05}
# Fit an intercept only model
rocket_fit_reduced <- lm(strength ~ 1, data = rocket)
mean(rocket$strength)  # compare to estimated intercept  #<<
anova(rocket_fit_reduced, rocket_fit)  # compare models  #<<
```

]


---

# Rule of thumb `r set.seed(101); emo::ji("thumb")`

.font250[

Using $Esimtate \pm 2 \times SE$ for an approximate 95% CI is incredibly robust!

]

.code125[

```{r robust}
sapply(c(10, 20, 30, 50, Inf), function(x)  #<<
  qt(0.975, df = x))                        #<<
```

]



---

# Estimation versus prediction

.font175[

* Regression analysis is really a problem of estimating a conditional mean or expectation, for example, in SLR we have $$E\left(Y|X\right) = \beta_0 + \beta_1 X$$

* Here, $E\left(Y|X\right)$ corresponds to the average value of the response $Y$ for all units in the population with a specific $X$ value

]


---

# Estimation versus prediction

.font200[

Suppose we want to estimate the mean strength of .bold[all rockets] with an age of 15 weeks

]

--

.center.font125.content-box-yellow[

How do we estimate $E\left(strength | age = 15\right)$?

]

--

.font200.center[

$\widehat{Y} = 2627.822 − 37.154(15) = 2070.512$ (psi)

]


---

# CI for the mean response

.font150[

* We can estimate the mean response given $X = X_0$ as $\widehat{Y}_0 = \widehat{\beta}_0 + \widehat{\beta}_1 X_0$ 

]

--

.font150[

* How can we construct a CI for $\widehat{Y}$? `r emo::ji("thinking")`

]

--

.font150[

* What is the sampling distribution of $\widehat{Y}_0$? `r emo::ji("thinking")`

]

--

.font150[

$$\frac{\widehat{Y}_0 - E\left(\widehat{Y}_0\right)}{\widehat{SE}\left(\widehat{Y}_0\right)} \sim t_{n-2}$$

]


---

# CI for the mean response

.font150[

* $E\left(\widehat{Y}_0\right) = \beta_0 + \beta_1 x_0$

* $\widehat{SE}\left(\widehat{Y}_0\right) = MSE\left[\frac{1}{n} + \frac{\left(X_0 - \bar{X}\right)^2}{S_{xx}}\right]$

]

--

.font150[

A $1 - \alpha$ CI for the mean response at $X = X_0$ is given by $$\widehat{Y}_0 \pm t_{1 - \alpha/2, n-2} MSE\left[\frac{1}{n} + \frac{\left(X_0 - \bar{X}\right)^2}{S_{xx}}\right]$$

]

--

.font125[.center[.red[At what point is this interval smallest?]]]


---

# Rocket propellant example

.code150[

```{r rocket-conf-band-01}
# Confidence interval for the mean response 
# at age = 15
new_data <- data.frame(age = 15)
predict(rocket_fit, newdata = new_data, 
        interval = "confidence")
```

]


---
class: clear

```{r rocket-conf-band-02, fig.width=6, fig.asp=0.618, out.width="80%"}
# Plot a 95% (pointwise) confidence band
plotFit(rocket_fit, interval = "confidence")
abline(v = 15, col = "red2")
```


---
class: clear

```{r rocket-conf-band-03, fig.width=6, fig.asp=0.618, out.width="80%"}
# Extrapolation
plotFit(rocket_fit, interval = "confidence", 
        shade = TRUE, xlim = c(-20, 100))
```


---

# Your turn

.font150[

Using the crystal weight example, compute a 90% confidence interval for the mean response at $X = 20$ hours. Plot a (pointwise) 90% confidence band using the `plotFit()` function. See `?predict.lm` and `?plotFit` for help.

]


---

# Solution

.code125[

```{r crystal-conf-band-01, eval=FALSE}
# Confidence interval for the mean response 
# at age = 15
new_data <- data.frame(time = 20)
predict(crystal_fit, newdata = new_data, 
        interval = "confidence", level = 0.9)  #<<

# Plot the (pointwise) confidence band around the 
# fitted regression line
plotFit(crystal_fit, interval = "confidence", 
        cex = 1.4, pch = 19, shade = TRUE, 
        col.conf = adjustcolor("red2", alpha.f = 0.5))
```

]


---

# Solution

```{r crystal-conf-band-02, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="80%"}
# Confidence interval for the mean response at age = 15
new_data <- data.frame(time = 20)
predict(crystal_fit, newdata = new_data, interval = "confidence",
        level = 0.9)  #<<

# Plot the (pointwise) confidence band around the fitted regression line
plotFit(crystal_fit, interval = "confidence", cex = 1.4, pch = 19, shade = TRUE,
        col.conf = adjustcolor("red2", alpha.f = 0.5))
```


---

# Estimation versus prediction

Suppose we want to predict the strength of a new rocket at an age of 15 weeks. Then we would simply plug $X = 15$ into the estimated regression equation to get a predicted value of $\widehat{Y} = 2627.822 − 37.154(15) = 2070.512$ (psi).

* For the **estimation problem**, we wanted to estimate the mean of the population of all rockets that are 15 weeks old 
    
* For the **prediction problem**, we want to predict the strength of a **single rocket** at 15 weeks

.font125.content-box-yellow[

In both cases, we use $\widehat{Y} = 2070.512$ as the predicted strength and as the estimate of the mean strength of of rockets that are 15 weeks old

]


---

# Estimation versus prediction

.font125[

* There is more uncertainty associated with predicting a single new observation (**Why?** `r emo::ji("thinking")`)

* For a given value of $X$, it is customary to compute **confidence intervals for an estimated mean response** and a **prediction interval for a single new response value**

* The idea of a prediction interval is to determine an interval that will contain a certain percentage of the population

    - Because a prediction interval is attempting to capture a single, random future response, as opposed to the mean of the conditional population, it will be wider than the associated confidence interval

]


---
class: clear, middle, center


.font300[

[Confidence intervals vs prediction intervals](https://stats.stackexchange.com/questions/16493/difference-between-confidence-intervals-and-prediction-intervals)

]


---

# Coefficient of determination

.font150[

* A useful performance metric in linear regression, called the *coefficient of determination*, is defined as $$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$

]

--

.font150[

* In linear regression, $R^2$ can be interpreted as the fraction of variance explained (FVE) (i.e., the proportion of the variation in $Y$ that can be explained by $X$)

    - $0 \le R^2 \le 1$

]


---

# Rocket propellant example

.code150[

```{r rocket-rsquared-01}
# Extract R-squared from the model summary
summary(rocket_fit)$r.squared
```

]


---

# Rocket propellant example

.code130[

```{r rocket-rsquared-02}
# ANOVA decomposition
anova(rocket_fit)
```

]


---

# Rocket propellant example

.code130[

```{r rocket-rsquared-03}
# Compute R-squared by hand
SSE <- anova(rocket_fit)["Residuals", "Sum Sq"]
SST <- sum((rocket$strength - 
              mean(rocket$strength)) ^ 2)
round(c(SSE, SST, 1 - SSE/SST), digits = 3)
```

]


---

# Your turn

.font250[

Compute the coefficient of determination (i.e., $R^2$) for the crystal weight example by hand .purple[and interpret its value].

]


---

# Solution

.code130[

```{r crystal-rsquared}
# Compute R-squared by hand
SSE <- anova(crystal_fit)["Residuals", "Sum Sq"]
SST <- sum((crystal$weight - 
              mean(crystal$weight)) ^ 2)
1 - SSE/SST
```

]

.font175[

Roughly `r scales::percent(summary(crystal_fit)$r.squared)` of the variability in the final weight of crystals is explained by the growth time. (At least in this sample.)

]


---

# Common misunderstandings about $R^2$

.font150[

* A high $R^2$ (i.e., near 1) indicates that a useful (i.e., accurate) prediction can be made

]

--

.font150[

* A high $R^2$ (i.e., near 1) indicates that the estimated regression line provides a good fit to the data

]

--

.font150[

* A small $R^2$ (i.e., near zero) indicates that $X$ and $Y$ are not related

]


---

# Other things to look out for `r emo::ji("eyes")`

.font150[

* $R^2$ will .red[always] increase when more terms are added to the model (**more on this in multiple linear regression**)

]

--

.font150[

* As the range of $X$ increases/decreases, $R^2$ also generally increases/decreases

]

--

.font150[

* $R^2$ does not indicate the **appropriateness** of a linear model

]

--

.font250.center.blue[So why even use the coefficient of determination?]


---

# Coefficient of correlation

.font200[

In SLR, there is a connection between $R^2$ and the Pearson correlation coefficient between $X$ and $Y$: $$r = \pm \sqrt{R^2}$$

]

.font200.center[*r* will have the same sign as the estimated slope!]


---

# Rocket propellant example

.font125[

```{r rocket-08}
# Compute coefficient of correlation
(r_squared <- summary(rocket_fit)$r.squared)
sqrt(r_squared)
cor(rocket)  # compare with correlation coefficient
```

]


---

# Crystal weight example 

.font200[

For the crystal weight data, rather than estimating the mean weight for crystals grown for 10 weeks, suppose we wanted to estimate how long we should grow the crystals such the average weight will be 10 grams. Use your intuition to derive a point estimate for this value. 

]
