<!DOCTYPE html>
<html>
  <head>
    <title>BANA 7052: Lecture 01</title>
    <meta charset="utf-8">
    <meta name="author" content="Brandon M. Greenwell" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/hygge.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/fontawesome-all.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# BANA 7052: Lecture 01
## Association and Simple Linear Regression
### Brandon M. Greenwell
### 16 October, 2018

---

class: clear, center, middle



&lt;img src="images/curve-fitting.png" width="40%" style="display: block; margin: auto;" /&gt;


---

# Reading assignment

.font200[

* Chapter: 1

    - Sections: 1.1-1.8

* Main topics:

    - Association üò¥
    
    - Simple linear regression
  
]


---

# Prerequisites

.scrollable[


```r
# List of required (CRAN) packages
pkgs &lt;- c(
  "animation",   # for pre-built statistical animations
  "dplyr",       # for data wrangling
  "ggplot2",     # for drawing nicer graphics
  "gridExtra",   # for grid.arrange() function
  "HistData",    # for historical data sets
  "investr",     # for inverse estimation
  "magick",      # for working with images
  "roundhouse",  # for pure awesomeness
  "tibble"       # for nicer data frames
)

# Install required (CRAN) packages
for (pkg in pkgs) {
  if (!(pkg %in% installed.packages()[, "Package"])) {
    install.packages(pkg)
  }
}
```

]


---
class: clear, middle, center

&lt;img src="images/read-the-book.jpg" width="70%" style="display: block; margin: auto;" /&gt;


---

# Statistical relationships

.code110[


```r
# Load required packages
library(ggplot2)
library(tibble)

# Generate some random data
set.seed(101)
d &lt;- tibble(
  x = seq(from = -2, to = 2, length = 100),
  y1 = 1 + x + rnorm(length(x)),
  y2 = 1 + x^2 + rnorm(length(x)),
  y3 = 1 + x^3 + rnorm(length(x)),
  y4 = 1 + rnorm(length(x))
)
```

]


---

# Statistical relationships

.code90[


```r
# Construct scatterplots
p1 &lt;- ggplot(d, aes(x = x, y = y1)) +
  geom_point(alpha = 0.9) + 
  labs(y = "y", title = "Linear")
p2 &lt;- ggplot(d, aes(x = x, y = y2)) +
  geom_point(alpha = 0.9) + 
  labs(y = "y", title = "Quadratic")
p3 &lt;- ggplot(d, aes(x = x, y = y3)) +
  geom_point(alpha = 0.9) + 
  labs(y = "y", title = "Cubic")
p4 &lt;- ggplot(d, aes(x = x, y = y4)) +
  geom_point(alpha = 0.9) + 
  labs(y = "y", title = "None")

# Display plots in a grid
gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)
```

]


---

# Statistical relationships

&lt;img src="lecture-01_files/figure-html/statistical-relationships-04-1.png" width="80%" style="display: block; margin: auto;" /&gt;


---

# Pearson correlation

&lt;img src="lecture-01_files/figure-html/pearson-correlation-01-1.png" width="80%" style="display: block; margin: auto;" /&gt;


---
class: clear, middle, center

&lt;img src="lecture-01_files/figure-html/pearson-correlation-02-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

# Pearson correlation

.font160[

* The (Pearson) correlation coefficient (also called the *Pearson product-moment correlation coefficient*) between two random variables `\(X\)` and `\(Y\)` is given by 

`$$Cor\left(X, Y\right) = \rho = \frac{Cov\left(X,Y\right)}{\sigma_X\sigma_Y}$$`

* Given a sample `\(\left\{\left(x_i, y_i\right)\right\}_{i=1}^n\)`, we estimate `\(\rho\)` with `\(r = S_{xy} / \sqrt{S_{xx}S_{yy}}\)`

  - Where, for example, `\(S_{xy} = \sum_{i=1}^n\left(x_i - \bar{x}\right)\left(y_i - \bar{y}\right)\)`

]


---

# Pearson correlation

.font130[

* It is common to test the hypothesis `\(H_0: \rho = 0\)` vs. `\(H_1: \rho \ne 0\)`

    - Rejecting `\(H_0\)` is only evidence that `\(\rho\)` is **not exactly zero**

    - A *p*-value **does not measure the magnitude of the (linear) association**

    - Sample size affects the p-value! üò±

* There is a formal relationship between Pearson's correlation and the *simple linear regression* (SLR) model

    - Simple linear relationships can be described by a *slope* and *intercept*

]


---

# Interpretting correlation(s) ‚ö†Ô∏è

.font150[

* Correlation between `\(X\)` and `\(Y\)` does not imply causation

    - It does not imply the direction of any possible **causal relationship** between `\(X\)` and `\(Y\)`
    
    - There might be a third (*lurking*) variable that is the cause of changes in both variables (i.e., the association between `\(X\)` and `\(Y\)` might be indirect)
    
* The Pearson correlation only measures the **linear association **between `\(X\)` and `\(Y\)`

]


---
class: clear, middle, center

# Correlation and causation?

&lt;iframe width="800" height="450" src="https://www.youtube.com/embed/ntnalq-2nNU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;



---
class: clear, center, middle

.font300[

[Fun with spurious correlations](http://www.tylervigen.com/spurious-correlations)

]


---
class: clear, center, middle

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="images/correlation-causation-comic.png" alt="If you get the joke in this comic, then you probably undestand enough about correlation!" width="100%" /&gt;
&lt;p class="caption"&gt;If you get the joke in this comic, then you probably undestand enough about correlation!&lt;/p&gt;
&lt;/div&gt;


---

# Crystal weight example

.scrollable[


```r
data(crystal, package = "investr")  # ?investr::crystal
tibble::as_tibble(crystal)  # print a summary of the data
```

```
## # A tibble: 14 x 2
##     time weight
##    &lt;dbl&gt;  &lt;dbl&gt;
##  1     2   0.08
##  2     4   1.12
##  3     6   4.43
##  4     8   4.98
##  5    10   4.92
##  6    12   7.18
##  7    14   5.57
##  8    16   8.4 
##  9    18   8.81
## 10    20  10.8 
## 11    22  11.2 
## 12    24  10.1 
## 13    26  13.1 
## 14    28  15.0
```

]


---

# Your turn

.font130[

* Draw a scatter plot of `time` vs `weight` for the `crystal` data frame

    - Does there appear to be an association between these two variables?
    
    - Describe the nature of the association (if it exists)

* Use the `cor()` and `cor.test()` functions to assess the Pearson correlation between these two variables

    - **Hint:** Use `?cor` and `?cor.test` to see how to use these functions and for explanations of their respective output
    
    - Interpret the *p*-value in the output

]


---

# Solution


```r
# Scatterplot using base R graphics
# plot(crystal)  # OK
plot(weight ~ time, data = crystal)  # better
```

&lt;img src="lecture-01_files/figure-html/your-turn-01-01-1.png" width="70%" style="display: block; margin: auto;" /&gt;


---

# Solution


```r
# Scatterplot using ggplot2
ggplot(crystal, aes(x = time, y = weight)) +
  geom_point(size = 3)
```

&lt;img src="lecture-01_files/figure-html/your-turn-01-02-1.png" width="70%" style="display: block; margin: auto;" /&gt;


---

# Solution

.code125[


```r
cor(crystal)  # correlation matrix
```

```
##             time    weight
## time   1.0000000 0.9719016
## weight 0.9719016 1.0000000
```

```r
with(crystal, cor(time, weight))  # using with() 
```

```
## [1] 0.9719016
```

```r
cor(crystal$time, crystal$weight)  # using $
```

```
## [1] 0.9719016
```

]


---

# Solution

.code110[


```r
# Test for (linear) association
with(crystal, cor.test(time, weight, conf.level = 0.95))
```

```
## 
## 	Pearson's product-moment correlation
## 
## data:  time and weight
## t = 14.303, df = 12, p-value = 6.688e-09
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.9112032 0.9912976
## sample estimates:
##       cor 
## 0.9719016
```

]


---

# Which of the following statements are true?

.font170[

a. The Pearson correlation coefficient is a measure of linear association

b. A non-significant *p*-value for a Pearson correlation means no relationship

c. A negative Pearson correlation indicates a low degree of linear association

d. A random cloud of data implies a negative correlation.

]


---

# Which of the following statements are true?

.font170[

a. .green[The Pearson correlation coefficient is a measure of linear association]

b. .red[A non-significant *p*-value for a Pearson correlation means no relationship]

c. .red[A negative Pearson correlation indicates a low degree of linear association]

d. .red[A random cloud of data implies a negative correlation]

]


---
class: clear, middle

.font300.center[

"All models are wrong, but some are useful."

--- George E. Box ---

]


---

# Historical origins of regression

.font170[

* First developed by Sir Francis Galton in the later part of the 19-th century

* Studied heights of parents and their children

* Noted that the heights of children with tall/short parents tended to "revert" or "regress" to the mean of the group

* The term "regression" persists to this day!

]


---

# Historical origins of regression

.code100[


```r
# Scatterplot of Galton's height data
ggplot(HistData::Galton, aes(x = parent, y = child)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  labs(x = "Height of mid-parent (in)", y = "Height of child (in)")
```

]


---

# Historical origins of regression

&lt;img src="lecture-01_files/figure-html/galton-02-1.png" width="90%" style="display: block; margin: auto;" /&gt;


---
class: clear, middle, center

&lt;iframe width="800" height="450" src="https://www.youtube.com/embed/1tSqSMOyNFE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;


---

# Functional relationships

.code100[


```r
# Scatterplot of (fake) functional data
ggplot(data.frame(x = 1:10, y = 15*1:10), aes(x, y)) +
  geom_line() +
  geom_point(size = 3, color = "red2") +
  labs(x = "Hours worked (X)", y = "Rate of pay (Y)", 
       title = expression(paste("Relation: ", Y == 15 %*% X)))
```

]


---

# Functional relationships

&lt;img src="lecture-01_files/figure-html/functional-relation-02-1.png" width="70%" style="display: block; margin: auto;" /&gt;


---

# Statistical relationships

&lt;img src="lecture-01_files/figure-html/linear-relationships-1.png" width="90%" style="display: block; margin: auto;" /&gt;


---

# Examples of statistical relationships

.font175[

* Simple linear regression: `\(Y = \beta_0 + \beta_1 X + \epsilon\)`

]

--

.font175[

* Multiple linear regression: `\(Y = \beta_0 + \sum_{i=1}^p \beta_p X_p + \epsilon\)`

]

--

.font175[

* Polynomial regression: `\(Y = \beta_0 + \sum_{i=1}^p \beta_p X^p + \epsilon\)` 

]

--

.font175[

* Nonlinear regression: `\(Y = \beta_1 X / \left(\beta_2 + X\right) + \epsilon\)` 

]


---

# Still with me?


```r
roundhouse::kick("Chuck Norris caught all the Pok\uE9mon from a landline",
                 width = 40, type = 2, fps = 10)
```

&lt;img src="lecture-01_files/figure-html/roundhouse-02-1.gif" width="70%" style="display: block; margin: auto;" /&gt;


---

# Simple linear regression (SLR)

.font140[

* Data: `\(\left\{\left(X_i, Y_i\right)\right\}_{i=1}^n\)`

* Model: `\(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\)`

    - `\(Y_i\)` is a continuous response

    - `\(X_i\)` is a continuous predictor

    - `\(\beta_0\)`  is the intercept of the regression line (also called the *bias term*)

    - `\(\beta_1\)`  is the slope of the regression line
    
    - `\(\epsilon_i \stackrel{iid}{\sim} \left(0, \sigma^2\right)\)`

]


---

# Assumptions about the errors

.font170[

* For `\(i/j = 1, 2, \dots, n\)` and `\(i \ne j\)`

    1) `\(\quad E\left(\epsilon_i\right) = 0\)`

    2) `\(\quad Var\left(\epsilon_i\right) = \sigma^2\)` (homoscedacticity üò±)

    3) `\(\quad Cov\left(\epsilon_i, \epsilon_j\right) = 0\)` (independence)

]

--

.font170[

* Assumptions 1)-3) can be summarized as `\(\epsilon_i \stackrel{iid}{\sim} \left(0, \sigma^2\right)\)`

]


---

# Properites of SLR

.font150[

* Simple linear regression: `\(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\)`

    - Implies the model is **linear in the regression coefficients `\(\beta_0\)` and `\(\beta_1\)`**
    
* The error term is a random variable; hence, `\(Y_i\)` is also a random variable (**Why?** ü§î)

    - What is `\(E\left(Y_i|X_i\right)\)` and `\(Var\left(Y_i|X_i\right)\)`?
    
* `\(Cor\left(Y_i, Y_j\right) = 0\)` `\(\forall i \ne j\)` (**Why?** ü§î)

]


---

# SLR with normal errors

.font170[

* `\(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i, \quad i = 1, 2, \dots, n\)`

where `\(\epsilon_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)\)`

]

--

.red[**OR, EQUIVALENTLY**]

.font170[

* `\(Y_i \stackrel{indep.}{\sim} N\left(\beta_0 + \beta_1 X_i, \sigma^2\right), \quad i = 1, 2, \dots, n\)`

     - `\(\mu_i = \beta_0 + \beta_1 X_i\)` is called the *linear predictor* (LP)

]


---

# Interpretting the regression coefficients

.font130[

* `\(\beta_0\)` is the *y*-intercept (or *bias term*)

    - It represents the mean response when `\(X = 0\)`; that is `\(\beta_0 = E\left(Y|X = 0\right)\)`

    - In general, the intercept is of little practical interest (this is especially true in MLR or when zero is not a valid value of `\(X\)`)

* `\(\beta_1\)` is the slope of the regression line

    - The slope of a line represents a *rate of change*
    
    - It represents the average change in `\(Y\)` per one-unit increase in `\(X\)`

]


---
class: clear, middle, center

&lt;img src="images/slr-conditional-distribution.png" width="80%" style="display: block; margin: auto;" /&gt;


---
class: clear, middle, center

&lt;img src="lecture-01_files/figure-html/arsenic-conditional-distribution-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

# How do we estimate `\(\beta_0\)` and `\(\beta_1\)`?

--

.font120[

* Ideally, we want estimates of `\(\beta_0\)` and `\(\beta_1\)` that give us the "best fitting" line

    - But what is meant by "best fitting"?

]

--

.font120[

* The most common approach is to use the method of *least squares* (LS) estimation

* The LS estimates of `\(\beta_0\)` and `\(\beta_1\)` minimize the residual sum of squares (RSS):

`$$RSS\left(\beta_0, \beta_1\right) = \sum_{i=1}^n\left[Y_i = \left(\beta_0 + \beta_1 X_i\right)\right]^2 = \sum_{i=1}^n\left(Y_i - \beta_0 - \beta_1 X_i\right)^2$$`

* `\(Y_i - \left(\beta_0 + \beta_1 X_i\right) = \epsilon_i\)` are called the *residual errors*

]

--

.font120[

* Why minimize `\(\sum_{i=1}^n\epsilon_i^2\)`? Why not `\(\sum_{i=1}^n\left|\epsilon_i\right|\)` or something more general like `\(\sum_{i=1}^n\psi\left(\epsilon_i\right)\)`?

]


---
class: clear, middle, center

.font200[

How do me minimize `\(RSS\left(\beta_0, \beta_1\right) = \sum_{i=1}^n\left(Y_i - \beta_0 - \beta_1 X_i\right)^2\)`?

]

--

.font300.purple[

Calculus!

]


---
class: clear, middle, center

&lt;img src="images/calculus-joke.jpg" width="100%" style="display: block; margin: auto;" /&gt;


---

# Derive the LS estimates of `\(\beta_0\)` and `\(\beta_1\)`

&lt;img src="images/homer-simpson-smart.jpg" width="45%" style="display: block; margin: auto;" /&gt;


---

# The LS estimates of `\(\beta_0\)` and `\(\beta_1\)`

.font130[

* `\(\widehat{\beta}_1 = \frac{\sum_{i=1}^n\left(X_i - \bar{X}\right)\left(Y_i - \bar{Y}\right)}{\sum_{i=1}^n\left(X_i - \bar{X}\right)^2} = \frac{S_{xy}}{ S_{xx}} = \sum_{i=1}^n\left(\frac{X_i - \bar{X}}{\sum_{i=1}^n\left(X_i - \bar{X}\right)^2}\right)Y_i = \sum_{i=1}^n w_iY_i\)`
    
* `\(\widehat{\beta}_0 = \bar{Y} - \widehat{\beta}_1 \bar{X}\)`

]

--

.font130[

* Don't bother trying to memorize these formulas! ü§£

]

--

.font130[

* Once we start adding more predictors (.red[i.e., real life]), we lose the ability to easily find closed-form formulas like these and rely on numerical approximations to find them (e.g., in R or SAS)

* As will be proven later, the LS approach provides the .purple[*best linear unbiased estimators*] (BLUE) of `\(\beta_0\)` and `\(\beta_1\)`

]


---

# Visualizing least squares

.font175[

Take five minutes to run the following code (see `?animation::least.squares` for details):

]

.code125[


```r
animation::least.squares(ani.type = "slope")
animation::least.squares(ani.type = "intercept")
```

]

&lt;br&gt;
&lt;br&gt;

.font200.center.forestgreen.bold[

What do you think is happening here?

]


---
class: clear, middle, center

.font300[Fitting the SLR Model in R]


---

# The lm() function

.font200[

* The `lm()` function can be used to fit the SLR model (or any LM for that matter!)

* The statement `lm(y ~ x, data = df)` fits an SLR model by regressing `y` on `x`, where `y` and `x` are columns in `df`

* To suppress the intercept term, use `y ~ x - 1`

]


---

# Your turn

.font200[

Fit an SLR model to the crystal weight data using `weight` as the response and `time` as the predictor and interpret the estimated coefficients.

]


---

# Solution


```r
# Load the data (if not already loaded)
data(crystal, package = "investr")

# Fit an SLR model to the data
fit &lt;- lm(weight ~ time, data = crystal)
print(fit)  # print a basic summary
```

```
## 
## Call:
## lm(formula = weight ~ time, data = crystal)
## 
## Coefficients:
## (Intercept)         time  
##    0.001429     0.503429
```


---

# Solution

.scrollable[


```r
# More verbose summary
summary(fit)
```

```
## 
## Call:
## lm(formula = weight ~ time, data = crystal)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.96371 -0.73464  0.05629  0.89193  1.40800 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.001429   0.599387   0.002    0.998    
## time        0.503429   0.035197  14.303 6.69e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.062 on 12 degrees of freedom
## Multiple R-squared:  0.9446,	Adjusted R-squared:   0.94 
## F-statistic: 204.6 on 1 and 12 DF,  p-value: 6.688e-09
```

]


---

# Solution

.font200[

* The average final weight of crystals increases by an estimated 0.503 grams for every one-hour increase in growth time

* Does interpreting the intercept make sense in this problem?

]


---

# The fitted SLR model

.font200[

* The fitted model is given by `\(\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1 X\)`

* We refer to `\(\widehat{Y}\)` as the predicted value

* The fitted values: `\(\widehat{Y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 X_i, \quad i = 1, 2, \dots, n\)`

* Use `fitted()` to obtain the fitted values

]


---

# The fitted SLR model

.code110[


```r
head(cbind(crystal, "fitted_values" = fitted(fit)))
```

```
##   time weight fitted_values
## 1    2   0.08      1.008286
## 2    4   1.12      2.015143
## 3    6   4.43      3.022000
## 4    8   4.98      4.028857
## 5   10   4.92      5.035714
## 6   12   7.18      6.042571
```

]


---

# Residuals: `\(Y_i - \widehat{Y}_i\)`

.code100[


```r
# Residual plot
ggplot(data = crystal, aes(x = time, y = weight)) +
  geom_point(size = 3) +
* geom_smooth(method = "lm", formula = y ~ x,
*             se = FALSE, alpha = 0.5) +
  geom_segment(aes(x = time, y = fitted(fit), 
                   xend = time, yend = weight), 
               alpha = 0.75, col = "red2", linetype = "solid") +
  theme_light() +
  xlab("Time (weeks)") +
  ylab("Weight (grams)") +
  ggtitle("Crystal weight data")
```

]


---

# Residuals: `\(Y_i - \widehat{Y}_i\)`

&lt;img src="lecture-01_files/figure-html/crystal-residuals-02-1.png" width="90%" style="display: block; margin: auto;" /&gt;


---

# Your turn

.font150[

1. Simulate `\(Y_i \stackrel{indep.}{\sim} N\left(\mu = 1 + 2X_i, \sigma = 1.2\right)\)` for `\(i = 1, 2, \dots, 30\)`, where `\(X_i \stackrel{iid}{\sim} U\left(0, 5\right)\)`. **Hint:** use `runif()` to simulate from a uniform distribution (see `?runif` for details).

2. Fit an SLR model to the data and estimate the slope and intercept. Do the estimated slope and intercept match closely with the true values? What happens to the estimates if we generate the response from a `\(N\left(\mu = 1 + 2X_i, \sigma = 0.2\right)\)` distribution instead?

]


---

# Solution

.code125[


```r
# Simulate data
set.seed(1105)  # for reproducibility
x &lt;- runif(30, min = 0, max = 5)
y &lt;- rnorm(30, mean = 1 + 2*x, sd = 1.2)
```

]


---

# Solution

.code125[


```r
# Simple scatterplot
plot(x, y)
```

&lt;img src="lecture-01_files/figure-html/your-turn-03-02-1.png" width="70%" style="display: block; margin: auto;" /&gt;

]


---

# Solution

.code125[


```r
# Fit an SLR model
lm(y ~ x)
```

```
## 
## Call:
## lm(formula = y ~ x)
## 
## Coefficients:
## (Intercept)            x  
##       1.341        1.871
```

]


---

# Properties of the residuals

.font160[

* `\(\sum_{i=1}^n e_i = 0\)` (.purple[**Why?**] ü§î)

* `\(\sum_{i=1}^n e_i^2\)` is a minimum

* `\(\sum_{i=1}^n X_ie_i = 0\)`

* `\(\sum_{i=1}^n \widehat{Y}_ie_i = 0\)`

* The LS regression line passes through the point `\(\left(\bar{X}, \bar{Y}\right)\)` (i.e., the center of the training data)

]


---
class: clear, middle

.font300.red.center[

What are the unknown parameters of the SLR model?

]

--

.font150[

* Assuming `\(\epsilon_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)\)`, we have an additional (unknown) parameter: `\(\sigma^2\)`

* How do we estimate `\(\sigma^2\)` (LS estimation only provides estimates for the slope and intercept)?

]


---

# Maximum likelihood estimation

.font125[

Since `\(Y_i \stackrel{indep.}{\sim} N\left(\beta_0 + \beta_1X_i, \sigma^2\right)\)`, the *likelihood function* for the data (as a function of `\(\beta_0\)`, `\(\beta_1\)`, and `\(\sigma^2\)`) is given by 

`$$L\left(\beta_0, \beta_1, \sigma^2\right) = \prod_{i=1}^nf\left(Y_i; \beta_0 + \beta_1X_i, \sigma^2\right),$$` where `$$f\left(Y_i; \beta_0 + \beta_1X_i, \sigma^2\right) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{Y_i - \beta_0 - \beta_1 X_i}{2\sigma^2}\right)$$` is the *probability density function* (PDF) of a normal distribution with mean `\(\beta_0 + \beta_1X_i\)` and variance `\(\sigma^2\)`.

]


---

# Maximum likelihood estimation

.font175[

* Maximizing the likelihood is the same as maximizing the *log-likelihood* `\(l = \log\left(L\right)\)` (**Why?** ü§î)

* The full log-likelihood is given by 

`$$l = -\frac{n}{2}\log\left(2\pi\right) - \frac{n}{2}\log\left(\sigma^2\right) - \frac{1}{2\sigma^2}\sum_{i=1}^n\left(Y_i - \beta_0 - \beta_1X_i\right)^2$$`

]


---

# Maximum likelihood estimation

.font175[

Maximum likelihood (ML) estimates of `\(\beta_0\)`, `\(\beta_1\)`, and `\(\sigma^2\)` can be found by equating the (partial) derivatives of `\(l\)` to zero:

* `\(\frac{\partial l}{\partial \beta_0} = \frac{1}{\sigma^2}\sum_{i=1}^n\left(Y_i - \beta_0 - \beta_1X_i\right)\)`
    
* `\(\frac{\partial l}{\partial \beta_1} = \frac{1}{\sigma^2}\sum_{i=1}^nX_i\left(Y_i - \beta_0 - \beta_1X_i\right)\)`
    
* `\(\frac{\partial l}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{\sigma^4}\sum_{i=1}^n\left(Y_i - \beta_0 - \beta_1X_i\right)^2\)`

]


---

# Maximum likelihood estimation

.font125[

* As it turns out, the ML estimates of `\(\beta_0\)` and `\(\beta_1\)` (under our current assumptions) are the same as the corresponding LS estimates

* ML estimation, however, provides us with an estimate of the error variance `\(\sigma^2\)` 
`$$\widehat{\sigma}_{MLE}^2 = \frac{1}{n}\sum_{i=1}^n e_i^2$$`

* However, since `\(\widehat{\sigma}_{MLE}^2\)` is a biased estimate for the error variance, we use an adjusted estimate

`$$\widehat{\sigma}^2 = \frac{1}{n-2}\sum_{i=1}^n e_i^2 = MSE$$`

]


---

# Your turn

.font200[

Use the `fitted()`, `residuals()`, and `sigma()` functions to obtain the fitted values, residuals, and RMSE, respectively, for the crystal weight example. Confirm the formula for the residuals.

]


---

# Rocket propellant example <i class="fas  fa-rocket fa-spin "></i>

.font140[

A rocket motor is manufactured by bonding an igniter propellant and a sustainer propellant together inside a metal housing. The shear strength of the bond ( `\(Y\)` )
between the two types of propellant is an important quality characteristic. It is suspected that the shear strength is related to the age ( `\(X\)` ) of the batch of
sustainer propellant. `\(n = 20\)` observations on shear strength (measured in psi) and age (measured in weeks) are available in the file `rocket.csv`. We'll use [this script](https://github.com/bgreenwell/uc-bana7052/blob/master/code/rocket.R) to analyze the data with simple linear regression.

]


---
class: clear, middle, center

&lt;img src="lecture-01_files/figure-html/quittin-time-1.png" width="100%" style="display: block; margin: auto;" /&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
