---
title: "BANA 7052: Lecture 05"
subtitle: "Regression Models for Quantitative and Qualitative Predictors"
author: "Brandon M. Greenwell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: [default, metropolis, metropolis-fonts, hygge, "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: clear, middle, center

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)

# Global chunk options
knitr::opts_chunk$set(
  cache = FALSE,
  echo = TRUE,
  dev = "svglite",
  fig.align = "center",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

# Bitmoji id
my_id <- "1551b314-5e8a-4477-aca2-088c05963111-v1"

# Load required packages
library(ggplot2)
```

.font300[

[R code for these slides](https://github.com/bgreenwell/uc-bana7052/blob/master/code/lecture-05.R)

]


---
class: clear, middle

.font150[

* Required reading

    - Chapters: 3, 6, and 8

    - Sections: 3.9, 6.1, and 8.1-8.7

* Main topics:

    - Regression Models for Quantitative and Qualitative Predictors (8.1-8.7)
  
    - Transformations (3.9 and 6.1)

]


---

# Prerquisites

.scrollable[

```{r prerequisites, eval=FALSE}
# List of required (CRAN) packages
pkgs <- c(
  "ggplot2",  # for drawing nicer graphics
  "lattice",  # for drawing nicer graphics
  "MASS",     # for boxcox() function
  "tibble"    # for nicer data frames
)

# Install required (CRAN) packages
for (pkg in pkgs) {
  if (!(pkg %in% installed.packages()[, "Package"])) {
    install.packages(pkg)
  }
}
```

]


---
class: clear, center, middle

```{r lets-go, echo=FALSE, out.width="70%"}
set.seed(4); RBitmoji::plot_comic(my_id, tag = "lets go")
```


---

# Categorical predictors

.font115[

* Also called .magenta[*qualitative*] predictors or .magenta[*factors*]

    - [Free DataCamp exercise](https://campus.datacamp.com/courses/free-introduction-to-r/chapter-4-factors-4?ex=4)

]

--

.font115[

* [Wikipedia states that](https://en.wikipedia.org/wiki/Categorical_variable) "In statistics, a categorical variable is a variable that can take on one of a limited, and usually fixed number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property."

    - Examples include gender (i.e., male/female), **zip code**, political affiliation, etc.

* In regression, we typically use .purple.bold[indicator variables] that take on the values 0 and 1 to identify the classes of a categorical variable

]


---

# Cutting tool example

.font120[

Suppose we want to relate the effective life of a cutting tool ( $Y$ ) used on a lathe to the lathe speed in revolutions per minute ( $X_1$ ) and type of cutting tool used ( $X_2$ ).  

* Tool type is categorical variable and can be represented using an indicator variable of the form: $$X_2 = \begin{cases} 0, & \quad \text{if tool A} \\ 1, & \quad \text{if tool B} \end{cases}$$

* If a first-order model is appropriate, then $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$

]

--

.font150.center.purple.bold[This implies two separate models!]


---

# Cutting tool example

.font150[

$$Y = \begin{cases} \beta_0 + \beta_1 X_1 + \epsilon, & \quad \text{if tool A} \\ \left(\beta_0 + \beta_2\right) + \beta_1 X_1 + \epsilon, & \quad \text{if tool B} \end{cases}$$

]

--

```{r cutting-tool-01, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="70%"}
n <- 20
set.seed(1213)
df <- tibble::tibble(
  x1 = rnorm(n),
  x2 = rep(c("Tool A", "Tool B"), each = n/2),
  y = 1 + 2*x1 + 3*ifelse(x2 == "Tool A", yes = 0, no = 1) + rnorm(n)
)
ggplot(df, aes(x = x1, y = y, color = as.factor(x2))) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  theme_light() +
  labs(x = expression(X[1]), y = expression(Y)) +
  annotate("text", x = -1.5, y = 6, label = "Hypothetical data", size = 6) +
  guides(color = guide_legend(title = "")) +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank())
```


---
class: clear, middle, center

```{r indicator-variables, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/indicator-variables.png")
```


---

# More than two categories

* In general, a categorical variable with $K$ levels requires $K - 1$ indicator variables

* For example, if there were three different tool types (i.e., A, B, and C), then the previous regression model would become: $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \epsilon,$$ where $$X_2 = \begin{cases} 1, & \quad \text{if tool B} \\ 0, & \quad \text{otherwise} \end{cases}$$ and $$X_3 = \begin{cases} 1, & \quad \text{if tool C} \\ 0, & \quad \text{otherwise} \end{cases}$$


---
class: clear, center, middle

```{r dummy-encoding, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/dummy-encoding.png")
```


---

# Categorical variables in R

.code125[

```{r categorical-variables-01}
# Categorical variable
(dow <- c("Mon", "Tue", "Wed", "Thu", "Fri"))
class(dow)
```

]


---

# Categorical variables in R

.code125[

```{r categorical-variables-02}
# Coerce to a factor; needed for use in lm()
(dow2 <- as.factor(dow))
class(dow2)
```

]


---

# Categorical variables in R

.code110[

```{r categorical-variables-03}
# R will handle dummy encoding for you
model.matrix( ~ dow2)
```

]


---

# Your turn

.font150[

The actual cutting tool data are available in [here](https://bgreenwell.github.io/uc-bana7052/data/cutting_tool.csv). Fit a linear regression model using `Hour` as the response and `rpm` (quantitative) and `ToolType` (qualitative with two categories) as the predictors of interest. Does the model fit seem reasonable? What do you conclude about the relationship between `Hour` and `rpm` for each category of `ToolType`?

]


---

# Solution

.font300[[R code](https://github.com/bgreenwell/uc-bana7052/blob/master/code/cutting-tool.R)]


---
class: clear, middle

.code125[

```{r cutting-tool-02}
# Load required packages
library(broom)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(lattice)

# Load the data
url <- paste0("https://bgreenwell.github.io/",
              "uc-bana7052/data/cutting_tool.csv")
cutting_tool <- read.csv(url)
```

]


---
class: clear, middle

.code125[

```{r cutting-tool-03}
# Inspect data
head(cutting_tool)
```

]


---
class: clear, middle

.code125[

```{r cutting-tool-04}
# Check column types
sapply(cutting_tool, class)

# Inspect ToolType column
head(cutting_tool$ToolType)
```

]


---
class: clear, middle

.code125[

```{r cutting-tool-05}
# How will R encode ToolType?
head(model.matrix( ~ rpm + ToolType, 
                   data = cutting_tool))
```

]


---
class: clear, middle

```{r cutting-tool-06, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
ggplot(cutting_tool, aes(x = rpm, y = Hour, group = ToolType)) +
  geom_point(aes(colour = ToolType), size = 3) +
  # geom_smooth(method = "lm", formula = y ~ x, se = FALSE,
  #             color = "black") +
  theme_light()
```


---
class: clear, middle

```{r cutting-tool-07, highlight.output=12:13}
# Regression model
summary(fit <- lm(Hour ~ rpm + ToolType, data = cutting_tool))
```


---
class: clear, middle

```{r cutting-tool-08, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# FItted regression line(s)
ggplot(augment(fit), aes(x = rpm, y = .fitted)) +
  geom_point(aes(y = Hour, colour = ToolType), size = 3) +
  geom_line(aes(group = ToolType), color = "black") +
  labs(x = "rpm", y = "Hour") +
  theme_light()
```


---
class: clear, middle

.font200[

Estimated regression equation:

]

.font170[

$$\widehat{\text{Hour}} = \begin{cases} 36.986 - 0.027 \text{rpm}, & \quad \text{tool A} \\ 51.990 - 0.027 \text{rpm}, & \quad \text{tool B} \end{cases}$$

]


---
class: clear, middle, center

```{r thinking-picard, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/thinking-picard.jpg")
```


---

# Unequal slopes

.font150.center.bold[

What if the slopes are expected to differ?

]

--

.font150[

Continuing with the previous cutting tool example:

* To model differing slopes, we can include an **interaction** (i.e., the product) between the qualitative and quantitative variables: $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon$$

]

--

.font150.center.bold.blue[What models are implied here?]


---

# Cutting tool example

.font150[

$$Y = \begin{cases} \beta_0 + \beta_1 X_1 + \epsilon, & \quad \text{if tool A} \\ \left(\beta_0 + \beta_2\right) + \left(\beta_1 + \beta_3\right) X_1 + \epsilon, & \quad \text{if tool B} \end{cases}$$

]

```{r cutting-tool-unequal-slopes, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="70%"}
url <- "https://bgreenwell.github.io/uc-bana7052/data/cutting_tool.csv"
cutting_tool <- read.csv(url)
ggplot(cutting_tool, aes(x = rpm, y = Hour, color = ToolType)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  theme_light()
```


---

# Your turn

.font150[

Refit your previous regression model to the cutting tool data, but be sure to include an interaction between `rpm` and `ToolType`. **Hint:** use `:` to model an interaction between two predictors in `lm()` (i.e., `x1 + x2 + x1:x2`). Reinterpret your results. What are the two estimated prediction equations implied by this model? Use the general linear test approach to compare this model with the previous one. Which model do you conclude is "better"?

]


---

# Analysis of covariance (ANCOVA)

.font120[

* In many statistical studies, the goal is to compare two or more groups in terms of a continuous response $y$ (e.g., the two-sample $t$-test or ANOVA)

* Oftentimes, however, additional information in the form of a continuous variable $x$ may available to help in the comparison

    - Ideally, $x$ will be correlated with $y$
    
* Our main interest lies in comparing the populations, but we would like to take into account the additional information contained in $x$

    - In this case, we call $x$ a covariate

* We'll illustrate with an example

]


---

# Fruitfly example

.font125[

**It has been established that increased reproduction reduces longevity in female fruit flies. A study was conducted to see if the same effect exists for male fruit flies** (Hanley and Shapiro, 1994). The experiment consisted of five groups: males forced to (i) live alone, (ii) to live with one pregnant female, (iii) to live with eight pregnant females, (iv) to live with one fertile female, and (v) to live with eight fertile females. The response of interest is `lifespan` (measured in days). Variables also measured were `thorax` length (mm), and the percentage of each day spent sleeping. For our analysis, we will only focus on two groups: control group of males living with one pregnant female and an experiment group of males living with one fertile female; these are stored in the factor variable `group` with levels `"control"` and `"treatment"`. 

]


---

# Loading the data

```{r fruitfly-01}
url <- "https://bgreenwell.github.io/uc-bana7052/data/fruitfly.csv"
fruitfly <- read.csv(url)
tibble::as_tibble(fruitfly)
```


---
class: clear, middle, center

```{r fruitfly-02, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
ggplot(fruitfly, aes(x = thorax, y = lifespan, color = group)) +
  geom_point(size = 3, alpha = 0.7) +
  theme_light() +
  labs(x = "Thorax length (mm)", y = "Lifespan (days)") +
  theme(legend.title = element_blank(), legend.position = c(0.1, 0.8))
```


---

# A two-sample $t$-test

.scrollable[

```{r fruitfly-03}
# Two-sample t-test
t.test(lifespan ~ group, data = fruitfly)

# Linear model (equivalent)
summary(lm(lifespan ~ group, data = fruitfly))
```

]


---

# Full model

.scrollable[

```{r fruitfly-04}
# Full model
fit1 <- lm(lifespan ~ thorax + group + thorax * group, 
           data = fruitfly)

# Print model summary
summary(fit1)
```

]


---

# Reduced model

.scrollable[

```{r fruitfly-05}
# Thorax only
fit2 <- lm(lifespan ~ thorax, data = fruitfly)

# Print model summary
summary(fit2)

# Compare models
anova(fit2, fit1)
```

]


---

# Testing for parallel slopes

.scrollable[

```{r fruitfly-06}
# Parallel regression lines
fit3 <- lm(lifespan ~ thorax + group, data = fruitfly)

# Print model summary
summary(fit3)

# Compare models
anova(fit3, fit1)
```

]


---
class: clear, middle, center

```{r fruitfly-07, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
library(lattice)
xyplot(lifespan ~ thorax, groups = group, data = fruitfly, pch = 19, 
       alpha = 0.5, type = c("p", "r"), xlab = "Thorax length (mm)",
       ylab = "Lifespan (days)", auto.key = list(corner = c(0, 1)))
```


---
class: clear, middle, center

.font300[

Transformations

]


---

# Variance stabilizing transformations

.font125[

* Homoscedasticity is often violated when the variance is functionally related to the mean

    - If not satisfied, the estimated regression coefficients will have larger standard errors (i.e., less precision)

* Applying a transformation to the response may alleviate the problem

* The strength of the transformation depends on the amount of curvature that is induced

* While subject matter expertise and prior knowledge can be used to help select an appropriate transformation, such transformation are typically selected **empirically**

]


---
class: clear, middle, center

```{r residuals-vs-x, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
set.seed(101)
n <- 100
df <- tibble::tibble(
  x = runif(n, min = 1, max = 5),
  y = 1 + 4*x + rnorm(n, sd = x)
)
fit <- lm(y ~ x, data = df)
df$r <- residuals(fit)
df$f <- fitted(fit)
ggplot(df, aes(f, r)) +
  geom_point(alpha = 0.7) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(x = "Fitted value", y = "Residual", 
       title = "Error variance increases with E(Y)") +
  theme_light()
```


---

# Useful transformations

.font150[

|  Relationship between $\sigma^2$ and $E\left(Y\right)$ | Transformation |
|:--|:--|
| $\sigma^2 \propto \text{constant}$ | $Y^\star = Y$ |
| $\sigma^2 \propto E\left(Y\right)$ | $Y^\star = \sqrt{Y}$ |
| $\sigma^2 \propto E\left(Y\right)\left[1 - E\left(Y\right)\right]$ | $Y^\star = \sin^{-1}\left(\sqrt{Y}\right)$ |
| $\sigma^2 \propto \left[E\left(Y\right)\right]^2$ | $Y^\star = Y^{-1/2}$ |
| $\sigma^2 \propto \left[E\left(Y\right)\right]^3$ | $Y^\star = Y$ |
| $\sigma^2 \propto \left[E\left(Y\right)\right]^4$ | $Y^\star = Y^{-1}$ |

]


---
class: clear, middle

.font125[

An electric utility is investigating the effect of the size of a single-family house and the type of air conditioning used in the house on the total electricity consumption during warm weather months

]

.code125[

```{r utility-01}
# Load the utility data
url <- paste0("https://bgreenwell.github.io/",
              "uc-bana7052/data/utility.csv")
utility <- read.csv(url)
head(utility, n = 5)  # print first 5 observations 
```

]


---
class: clear, middle

.code125[

```{r utility-02, eval=FALSE}
# Scatterplot
plot(Demand ~ Usage, data = utility, pch = 19, las = 1,
     col = adjustcolor("darkblue", alpha.f = 0.5))

# Fitted regression line
abline(fit <- lm(Demand ~ Usage, data = utility), 
       lwd = 2,
       col = adjustcolor("darkred", alpha.f = 0.5))
```

]


---
class: clear, middle

```{r utility-03, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# Scatterplot
plot(Demand ~ Usage, data = utility, pch = 19, las = 1,
     col = adjustcolor("darkblue", alpha.f = 0.5))
abline(fit <- lm(Demand ~ Usage, data = utility), 
       lwd = 2,
       col = adjustcolor("darkred", alpha.f = 0.5))
```


---
class: clear, middle

```{r utility-04, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# Residual plots
# par(mfrow = c(1, 2))
plot(
  x = fitted(fit), 
  y = rstudent(fit),
  pch = 19,
  las = 1,
  col = adjustcolor("darkblue", alpha.f = 0.5),
  xlab = "Fitted value",
  ylab = "Studentized residual"
)
abline(h = 0, lty = 2)
# plot(
#   x = utility$Usage, 
#   y = residuals(fit),
#   pch = 19,
#   las = 1,
#   col = adjustcolor("darkblue", alpha.f = 0.5),
#   xlab = "Usage (KWH)",
#   ylab = "Residual"
# )
# abline(h = 0, lty = 2)
```


---
class: clear, middle, center

.font300[

Which transformation(s) should we try? 

]

--

.font200[

(.red[This can involve a bit of trial and error!])

]


---
class: clear, middle

.code125[

```{r utility-05, eval=FALSE}
# Scatterplot
plot(sqrt(Demand) ~ Usage, data = utility, 
     pch = 19, las = 1,
     col = adjustcolor("darkblue", alpha.f = 0.5))

# Fitted regression line
abline(fit <- lm(sqrt(Demand) ~ Usage, data = utility),  #<<
       lwd = 2,
       col = adjustcolor("darkred", alpha.f = 0.5))
```

]


---
class: clear, middle

```{r utility-06, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# Scatterplot
plot(sqrt(Demand) ~ Usage, data = utility, pch = 19, las = 1,
     col = adjustcolor("darkblue", alpha.f = 0.5))
abline(fit <- lm(sqrt(Demand) ~ Usage, data = utility), 
       lwd = 2,
       col = adjustcolor("darkred", alpha.f = 0.5))
```


---
class: clear, middle

```{r utility-07, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# Residual plots
# par(mfrow = c(1, 2))
plot(
  x = fitted(fit), 
  y = rstudent(fit),
  pch = 19,
  las = 1,
  col = adjustcolor("darkblue", alpha.f = 0.5),
  xlab = "Fitted value",
  ylab = "Studentized residual"
)
abline(h = 0, lty = 2)
# plot(
#   x = utility$Usage, 
#   y = residuals(fit),
#   pch = 19,
#   las = 1,
#   col = adjustcolor("darkblue", alpha.f = 0.5),
#   xlab = "Usage (KWH)",
#   ylab = "Residual"
# )
# abline(h = 0, lty = 2)
```


---

# Box-Cox procedure

.font150[

* Suppose that we wish to transform $Y$ to correct for non-normality and/or non-constant variance (i.e., heteroscedasticity)

* A useful class of transformations is the power transformation, $Y^\lambda$ where $\lambda$ is a parameter to be estimated from the data

* The parameters of the regression model and $\lambda$ can be estimated simultaneously using the method of maximum likelihood estimation

]


---

# Box-cox procedure

.font150[

* The Box-Cox transformation uses $$Y_i^{\left(\lambda\right)} = \begin{cases} \frac{Y_i^\lambda - 1}{\lambda}, & \quad \lambda \ne 0 \\ \ln\left(Y_i\right), & \quad \lambda = 0 \end{cases}$$

* The model to be fit is $$Y_i^{\left(\lambda\right)} = \beta_0 + \beta_1 X_{i1} + \dots + \beta_{p-1} X_{i, p-1} + \epsilon_i \\ i = 1, 2, \dots, n$$

* In R, you can use [MASS::boxcox()](https://www.rdocumentation.org/packages/MASS/versions/7.3-50/topics/boxcox) or [car::boxCox()](https://www.rdocumentation.org/packages/car/versions/2.1-4/topics/boxCox) to find the "optimal" value of $\lambda$

]



---
class: clear, middle

.code125[

```{r boxcox-01, fig.wdith=6, fig.asp=0.618, out.width="60%"}
# Find optimal lambda value via ML estimation
bc <- MASS::boxcox(Demand ~ Usage, data = utility)
(lambda <- bc$x[which.max(bc$y)])
```

]


---
class: clear, middle

```{r boxcox-02, fig.wdith=6, fig.asp=0.618, out.width="80%"}
# Scatterplot and fitted model
utility$Demand2 <- (utility$Demand ^ lambda - 1) / lambda
plot(Demand2 ~ Usage, data = utility, pch = 19, las = 1,
     col = adjustcolor("darkblue", alpha.f = 0.5))
abline(fit <- lm(Demand2 ~ Usage, data = utility), 
       lwd = 2,
       col = adjustcolor("darkred", alpha.f = 0.5))
```


---
class: clear, middle

.code125[

```{r boxcox-03, fig.wdith=7, fig.asp=0.5, out.width="100%"}
par(mfrow = c(1, 2))  # side-by-side plots
plot(fit, which = 1:2)
```

]


---

# Transformations to linearize the model

.font150[

* Nonlinearity may be detected via plots or a *lack-of-fit* test 

* If a transformation of a nonlinear function can result in a linear function we say it is *intrinsically linear* or *transformably linear*

* Example: 

$$Y = \beta_0\exp{\left(\beta_1 X\right)}\epsilon$$ 

]

--

.font150[

$$\implies \ln{Y} = \ln{\beta_0} + \beta_1 X + \epsilon^\star \\ \implies Y^\star = \beta_0^\star + \beta_1 X + \epsilon^\star$$ 

]


---
class: clear, middle, center

```{r prototype-01, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="80%"}
curve(log10(x), xlab = "X", ylab = "Y", main = "General trend", 
      lwd = 10, axes = FALSE, lend = 2)
axis(1, labels = FALSE, tick = TRUE)
axis(2, labels = FALSE, tick = TRUE)
```

.font130[

Suggested transformations: $X^\star = \log_{10}\left(X\right)$ or $X^\star = \sqrt{X}$

]


---
class: clear, middle, center

```{r prototype-02, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="80%"}
curve(x^2, xlab = "X", ylab = "Y", main = "General trend", 
      lwd = 10, axes = FALSE, lend = 2)
axis(1, labels = FALSE, tick = TRUE)
axis(2, labels = FALSE, tick = TRUE)
```

.font130[

Suggested transformations: $X^\star = X^2$ or $X^\star = \exp{\left(X\right)}$

]


---
class: clear, middle, center

```{r prototype-03, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="80%"}
curve(1/x, xlab = "X", ylab = "Y", main = "General trend", 
      lwd = 10, axes = FALSE, xlim = c(0.1, 0.5), lend = 2)
axis(1, labels = FALSE, tick = TRUE)
axis(2, labels = FALSE, tick = TRUE)
```

.font130[

Suggested transformations: $X^\star = 1/X$ or $X^\star = \exp{\left(-X\right)}$

]


---

# Boston housing example 

.font150[

The data violate many classical assumptions like linearity, normality, and constant variance. Nonetheless, Harrison and Rubinfeld (1978) (using a combination of **transformations**, **significance testing**, and **grid searches**) were able to find a reasonable fitting model ( $R^2 = 0.81$ ):

]

.font100[

$$\widehat{\log\left(MV\right)} = 9.76 + 0.0063 RM^2 + 8.98\times10^{-5} AGE - 0.19\log\left(DIS\right) + 0.096\log\left(RAD\right) \\- 4.20\times10^{-4} TAX - 0.031 PTRATIO + 0.36\left(B - 0.63\right)^2 - 0.37\log\left(LSTAT\right) \\- 0.012 CRIM + 8.03\times10^{-5} ZN + 2.41\times10^{-4} INDUS + 0.088 CHAS \\- 0.0064 NOX^2$$

]


---

# Windmill example

.font200[

A research engineer is investigating the use of a windmill to generate electricity and has collected data on the DC output from this windmill ( $X$ ) and the corresponding wind velocity ( $Y$ ).

]


---
class: clear, middle

.code125[

```{r windmill-01}
# Load the windmill data
url <- paste0("https://bgreenwell.github.io/",
              "uc-bana7052/data/windmill.csv")
windmill <- read.csv(url)
head(windmill, n = 5)  # print first 5 observations 
```


]


---
class: clear, middle

.code115[

```{r windmill-02, eval=FALSE}
# Scatterplot
plot(Output ~ Velocity, data = windmill, pch = 19, las = 1,
     col = adjustcolor("darkblue", alpha.f = 0.5),
     xlab = "Wind velocity", ylab = "DC output",
     main = "Original data")

# Fitted regression line
abline(fit <- lm(Output ~ Velocity, data = windmill), 
       lwd = 2,
       col = adjustcolor("darkred", alpha.f = 0.5))
```

]


---
class: clear, middle

```{r windmill-03, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# Scatterplot
plot(Output ~ Velocity, data = windmill, pch = 19, las = 1,
     col = adjustcolor("darkblue", alpha.f = 0.5),
     xlab = "Wind velocity", ylab = "DC output",
     main = "Original data")

# Fitted regression line
abline(fit <- lm(Output ~ Velocity, data = windmill), 
       lwd = 2,
       col = adjustcolor("darkred", alpha.f = 0.5))
```


---
class: clear, middle

.code115[

```{r windmill-04, eval=FALSE}
# Residual plot
plot(fitted(fit), rstudent(fit), pch = 19, las = 1,
     col = adjustcolor("darkblue", alpha.f = 0.5),
     xlab = "Fitted value", ylab = "Studentized residual",
     main = "Original data")
abline(h = 0, lty = 2, 
       col = adjustcolor("darkred", alpha.f = 0.5))
```

]


---
class: clear, middle

```{r windmill-05, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# Residual plot
plot(fitted(fit), rstudent(fit), pch = 19, las = 1,
     col = adjustcolor("darkblue", alpha.f = 0.5),
     xlab = "Fitted value", ylab = "Studentized residual",
     main = "Original data")
abline(h = 0, lty = 2, 
       col = adjustcolor("darkred", alpha.f = 0.5))
```


---
class: clear, middle

.code115[

```{r windmill-06, eval=FALSE}
# Scatterplot
plot(Output ~ I(1/Velocity), data = windmill, pch = 19, las = 1,
     col = adjustcolor("darkblue", alpha.f = 0.5),
     xlab = "Wind velocity", ylab = "DC output",
     main = "Transformed data")

# Fitted regression line
abline(fit <- lm(Output ~ I(1/Velocity), data = windmill), 
       lwd = 2,
       col = adjustcolor("darkred", alpha.f = 0.5))
```

]


---
class: clear, middle

```{r windmill-07, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# Scatterplot
plot(Output ~ I(1/Velocity), data = windmill, pch = 19, las = 1,  #<<
     col = adjustcolor("darkblue", alpha.f = 0.5),
     xlab = "Wind velocity", ylab = "DC output",
     main = "Transformed data")

# Fitted regression line
abline(fit <- lm(Output ~ I(1/Velocity), data = windmill), 
       lwd = 2,
       col = adjustcolor("darkred", alpha.f = 0.5))
```


---
class: clear, middle

.code115[

```{r windmill-08, eval=FALSE}
# Residual plot
plot(fitted(fit), rstudent(fit), pch = 19, las = 1,  #<<
     col = adjustcolor("darkblue", alpha.f = 0.5),
     xlab = "Fitted value", ylab = "Studentized residual",
     main = "Transformed data")
abline(h = 0, lty = 2, 
       col = adjustcolor("darkred", alpha.f = 0.5))
```

]


---
class: clear, middle

```{r windmill-09, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# Residual plot
plot(fitted(fit), rstudent(fit), pch = 19, las = 1,
     col = adjustcolor("darkblue", alpha.f = 0.5),
     xlab = "Fitted value", ylab = "Studentized residual",
     main = "Transformed data")
abline(h = 0, lty = 2, 
       col = adjustcolor("darkred", alpha.f = 0.5))
```


---
class: clear, middle 

```{r windmill-10, highlight.output=12}
summary(fit)
```


---
class: clear, middle 

.center.font300[

`r emo::ji("warning")``r emo::ji("warning")``r emo::ji("warning")`

]

.font200[

The estimated coefficients still have the same properties, but only with respect to the transformed data, not the original data!

]

.center.font300[

`r emo::ji("warning")``r emo::ji("warning")``r emo::ji("warning")`

]


---

# Automated procedures

.font125[

* Alternating conditional expectations (ACE): uses the *alternating conditional expectations* algorithm to find the transformations of $Y$ and $X$ that maximize the proportion of variation in $Y$ explained by $X$

* Additivity and variance stabilization for regression (AVAS): estimates transformations of $X$ and $Y$ such that the regression of $Y$ on $X$ is approximately linear with constant variance

    - Both of these procedures are available in the [acepack](https://cran.r-project.org/package=acepack) package for R
    
* Multivariate adaptive regression splines (MARS): piecewise linear splines approach to multiple linear regression that automatically models nonlinearities and interactions between variables

]


---
class: clear, middle, center

```{r quittin-time, echo=FALSE, out.width="60%"}
RBitmoji::plot_comic(my_id, tag = "quittin")
```
