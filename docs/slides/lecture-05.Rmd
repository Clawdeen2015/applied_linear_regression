---
title: "BANA 7052: Lecture 05"
subtitle: "Regression Models for Quantitative and Qualitative Predictors"
author: "Brandon M. Greenwell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: [default, metropolis, metropolis-fonts, hygge, "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: clear, middle

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)

# Global chunk options
knitr::opts_chunk$set(
  cache = FALSE,
  echo = TRUE,
  dev = "svglite",
  fig.align = "center",
  # fig.width = 6,
  # fig.asp = 0.618,
  # out.width = "70%",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

# Bitmoji id
my_id <- "1551b314-5e8a-4477-aca2-088c05963111-v1"

# Load required packages
library(ggplot2)
```

.font150[

* Required reading

    - Chapters: 3, 6, and 8

    - Sections: 3.9, 6.1, and 8.1-8.7

* Main topics:

    - Regression Models for Quantitative and Qualitative Predictors (8.1-8.7)
  
    - Transformations (3.9 and 6.1)

]


---

# Prerquisites

.scrollable[

```{r prerequisites, eval=FALSE}
# List of required (CRAN) packages
pkgs <- c(
  "animation",   # for pre-built statistical animations
  "broom",       # for tidying the output from statistical models
  "dplyr",       # for data wrangling
  "ggplot2",     # for drawing nicer graphics
  "gridExtra",   # for grid.arrange() function
  "HistData",    # for historical data sets
  "investr",     # for inverse estimation
  "magick",      # for working with images
  "roundhouse",  # for pure awesomeness
  "tibble"       # for nicer data frames
)

# Install required (CRAN) packages
for (pkg in pkgs) {
  if (!(pkg %in% installed.packages()[, "Package"])) {
    install.packages(pkg)
  }
}
```

]


---
class: clear, center, middle

```{r lets-go, echo=FALSE, out.width="70%"}
set.seed(4); RBitmoji::plot_comic(my_id, tag = "lets go")
```


---

# Categorical predictors

.font115[

* Also called .magenta[*qualitative*] predictors or .magenta[*factors*]

    - [Free DataCamp exercise](https://campus.datacamp.com/courses/free-introduction-to-r/chapter-4-factors-4?ex=4)

]

--

.font115[

* [Wikipedia states that](https://en.wikipedia.org/wiki/Categorical_variable) "In statistics, a categorical variable is a variable that can take on one of a limited, and usually fixed number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property."

    - Examples include gender (i.e., male/female), **zip code**, political affiliation, etc.

* In regression, we typically use .purple.bold[indicator variables] that take on the values 0 and 1 to identify the classes of a categorical variable

]


---

# Cutting tool example

.font120[

Suppose we want to relate the effective life of a cutting tool ( $Y$ ) used on a lathe to the lathe speed in revolutions per minute ( $X_1$ ) and type of cutting tool used ( $X_2$ ).  

* Tool type is categorical variable and can be represented using an indicator variable of the form: $$X_2 = \begin{cases} 0, & \quad \text{if tool A} \\ 1, & \quad \text{if tool B} \end{cases}$$

* If a first-order model is appropriate, then $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$

]

--

.font150.center.purple.bold[This implies two separate models!]


---

# Cutting tool example

.font150[

$$Y = \begin{cases} \beta_0 + \beta_1 X_1 + \epsilon, & \quad \text{if tool A} \\ \left(\beta_0 + \beta_2\right) + \beta_1 X_1 + \epsilon, & \quad \text{if tool B} \end{cases}$$

]

--

```{r cutting-tool, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="70%"}
n <- 20
set.seed(1213)
df <- tibble::tibble(
  x1 = rnorm(n),
  x2 = rep(c("Tool A", "Tool B"), each = n/2),
  y = 1 + 2*x1 + 3*ifelse(x2 == "Tool A", yes = 0, no = 1) + rnorm(n)
)
ggplot(df, aes(x = x1, y = y, color = as.factor(x2))) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  theme_light() +
  labs(x = expression(X[1]), y = expression(Y)) +
  annotate("text", x = -1.5, y = 6, label = "Hypothetical data", size = 6) +
  guides(color = guide_legend(title = "")) +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank())
```


---
class: clear, middle, center

```{r indicator-variables, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/indicator-variables.png")
```


---

# More than two categories

* In general, a categorical variable with $K$ levels requires $K - 1$ indicator variables

* For example, if there were three different tool types (i.e., A, B, and C), then the previous regression model would become: $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \epsilon,$$ where $$X_2 = \begin{cases} 1, & \quad \text{if tool B} \\ 0, & \quad \text{otherwise} \end{cases}$$ and $$X_3 = \begin{cases} 1, & \quad \text{if tool C} \\ 0, & \quad \text{otherwise} \end{cases}$$


---
class: clear, center, middle

```{r dummy-encoding, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/dummy-encoding.png")
```


---

# Categorical variables in R

.code125[

```{r categorical-variables-01}
# Categorical variable
(dow <- c("Mon", "Tue", "Wed", "Thu", "Fri"))
class(dow)
```

]


---

# Categorical variables in R

.code125[

```{r categorical-variables-02}
# Coerce to a factor; needed for use in lm()
(dow2 <- as.factor(dow))
class(dow2)
```

]


---

# Categorical variables in R

.code110[

```{r categorical-variables-03}
# R will handle dummy encoding for you
model.matrix( ~ dow2)
```

]


---

# Your turn

.font150[

The actual cutting tool data are availble in [here](https://bgreenwell.github.io/uc-bana7052/data/cutting_tool.csv). Fit a linear regression model using `Hour` as the response and `rpm` (quantiative) and `ToolType` (qualitative with two categories) as the predictors of interest. Does the model fit seem reasonable? What do you conclude about the relationship between `Hour` and `rpm` for each category of `ToolType`?

]


---

# Solution

.font300[[R code](https://github.com/bgreenwell/uc-bana7052/blob/master/code/cutting-tool.R)]


---
class: clear, middle, center

```{r thinking-picard, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/thinking-picard.jpg")
```


---

# Unequal slopes

.font150.center.bold[

What if the slopes are expected to differ?

]

--

.font150[

Continuing with the previous cutting tool example:

* To model differing slopes, we can include an **interaction** (i.e., the product) between the qualitative and quantitative variables: $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon$$

]

--

.font150.center.bold.blue[What models are implied here?]


---

# Cutting tool example

.font150[

$$Y = \begin{cases} \beta_0 + \beta_1 X_1 + \epsilon, & \quad \text{if tool A} \\ \left(\beta_0 + \beta_2\right) + \left(\beta_1 + \beta_3\right) X_1 + \epsilon, & \quad \text{if tool B} \end{cases}$$

]

```{r cutting-tool-unequal-slopes, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="70%"}
url <- "https://bgreenwell.github.io/uc-bana7052/data/cutting_tool.csv"
cutting_tool <- read.csv(url)
ggplot(cutting_tool, aes(x = rpm, y = Hour, color = ToolType)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE)
```


---

# Your turn

.font150[

Refit your previous regression model to the cutting tool data, but be sure to include an interaction between `rpm` and `ToolType`. **Hint:** use `:` to model an intercation between two predictors in `lm()` (i.e., `x1 + x2 + x1:x2`). Reinterpret your results. What are the two estimated prediction equations implied by this model? Use the general linear test aproach to compare this model with the previous one. Which model do you conclude is "better"?

]


---

# Analysis of covariance (ANCOVA)

.font120[

* In many statistical studies, the goal is to compare two or more groups in terms of a continuous response $y$ (e.g., the two-sample $t$-test or ANOVA)

* Oftentimes, however, additional information in the form of a continuous variable $x$ may available to help in the comparison

    - Ideally, $x$ will be correlated with $y$
    
* Our main interest lies in comparing the populations, but we would like to take into account the additional information contained in $x$

    - In this case, we call $x$ a covariate

* We'll illustrate with an example

]


---

# Fruitfly example

.font125[

**It has been established that increased reproduction reduces longevity in female fruit flies. A study was conducted to see if the same effect exists for male fruit flies** (Hanley and Shapiro, 1994). The experiment consisted of five groups: males forced to (i) live alone, (ii) to live with one pregnant female, (iii) to live with eight pregnant females, (iv) to live with one fertile female, and (v) to live with eight fertile females. The response of interest is `lifespan` (measured in days). Variables also measured were `thorax` length (mm), and the percentage of each day spent sleeping. For our analysis, we will only focus on two groups: control group of males living with one pregnant female and an experiment group of males living with one fertile female; these are stored in the factor variable `group` with levels `"control"` and `"treatment"`. 

]


---

# Loading the data

```{r fruitfly-01}
url <- "https://bgreenwell.github.io/uc-bana7052/data/fruitfly.csv"
fruitfly <- read.csv(url)
tibble::as_tibble(fruitfly)
```


---
class: clear, middle, center

```{r fruitfly-02, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
ggplot(fruitfly, aes(x = thorax, y = lifespan, color = group)) +
  geom_point(size = 3, alpha = 0.7) +
  theme_light() +
  labs(x = "Thorax length (mm)", y = "Lifespan (days)") +
  theme(legend.title = element_blank(), legend.position = c(0.1, 0.8))
```


---

# A two-sample $t$-test

.scrollable[

```{r fruitfly-03}
# Two-sample t-test
t.test(lifespan ~ group, data = fruitfly)

# Linear model (equivalent)
summary(lm(lifespan ~ group, data = fruitfly))
```

]


---

# Full model

.scrollable[

```{r fruitfly-04}
# Full model
fit1 <- lm(lifespan ~ thorax + group + thorax * group, 
           data = fruitfly)

# Print model summary
summary(fit1)
```

]


---

# Reduced model

.scrollable[

```{r fruitfly-05}
# Thorax only
fit2 <- lm(lifespan ~ thorax, data = fruitfly)

# Print model summary
summary(fit2)

# Compare models
anova(fit2, fit1)
```

]


---

# Testing for parallel slopes

.scrollable[

```{r fruitfly-06}
# Parallel regression lines
fit3 <- lm(lifespan ~ thorax + group, data = fruitfly)

# Print model summary
summary(fit3)

# Compare models
anova(fit3, fit1)
```

]


---
class: clear, middle, center

```{r fruitfly-07, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
library(lattice)
xyplot(lifespan ~ thorax, groups = group, data = fruitfly, pch = 19, 
       alpha = 0.5, type = c("p", "r"), xlab = "Thorax length (mm)",
       ylab = "Lifespan (days)", auto.key = list(corner = c(0, 1)))
```


--- 

# More than three levels

.font125[

An electric utility is investigating the effect of the size of a single-family house and the type of air conditioning used in the house on the total electricity consumption during warm weather months

]


---
class: clear, middle, center

.font300[

Transformations

]


---

# Variance stabilizing transformations

.font125[

* Homoscedasticity is often violated when the variance is functionally related to the mean

    - If not satisfied, the estimated regression coefficients will have larger standard errors (i.e., less precision)

* Applying a transformation to the response may alleviate the problem

* The strength of the transformation depends on the amount of curvature that is induced

* While subject matter expertise and prior knowledge can be used to help select an appropriate transformation, such transformation are typically selected **empirically**

]


---
class: clear, middle, center

```{r residuals-vs-x, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
set.seed(101)
n <- 100
df <- tibble::tibble(
  x = runif(n, min = 1, max = 5),
  y = 1 + 4*x + rnorm(n, sd = x)
)
fit <- lm(y ~ x, data = df)
df$r <- residuals(fit)
ggplot(df, aes(x, r)) +
  geom_point(alpha = 0.7) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(x = "X", y = "Residual", title = "Error variance increases with X") +
  theme_light()
```


---

# Useful transformations

.font150[

|  Relationship between $\sigma^2$ and $E\left(Y\right)$ | Transformation |
|:--|:--|
| $\sigma^2 \propto \text{constant}$ | $Y^\star = Y$ |
| $\sigma^2 \propto E\left(Y\right)$ | $Y^\star = \sqrt{Y}$ |
| $\sigma^2 \propto E\left(Y\right)\left[1 - E\left(Y\right)\right]$ | $Y^\star = \sin^{-1}\left(\sqrt{Y}\right)$ |
| $\sigma^2 \propto \left[E\left(Y\right)\right]^2$ | $Y^\star = Y^{-1/2}$ |
| $\sigma^2 \propto \left[E\left(Y\right)\right]^3$ | $Y^\star = Y$ |
| $\sigma^2 \propto \left[E\left(Y\right)\right]^4$ | $Y^\star = Y^{-1}$ |

]
