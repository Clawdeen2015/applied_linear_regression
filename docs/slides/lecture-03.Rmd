---
title: "Multiple Linear Regression"
subtitle: "Lecture 03"
author: "Brandon M. Greenwell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: [default, metropolis, metropolis-fonts, hygge, "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: clear, middle, center

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)

# Global chunk options
knitr::opts_chunk$set(
  cache = TRUE,
  echo = TRUE,
  dev = "svglite",
  fig.align = "center",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

# Bitmoji id
my_id <- "1551b314-5e8a-4477-aca2-088c05963111-v1"

# Load required packages
library(dplyr)
```

```{r linear-algebra-tweet, echo=FALSE, out.width="60%"}
knitr::include_graphics("images/linear-algebra-tweet.png")
```


---
# Reading assignment

.font200[

* Chapters: 5; 6

    - Sections: 5.9-5.11; 6.1-6.6

* Main topics:

    - Regression in matrix form (5.9-5.11)
   
    - Multiple linear regression (6.1-6.4)

  
]


---

# Prerequisites

.scrollable.code120[

```{r prerequisites, eval=FALSE}
# List of required (CRAN) packages
pkgs <- c(
  "GGally",  # for gggplot2 extensions
  "pdp",     # for (corrected) Boston housing data
  "plotly",  # for interactive plots
  "tibble",  # for nicer data frames
  "vip"      # for variable importance plots
)

# Install required (CRAN) packages
for (pkg in pkgs) {
  if (!(pkg %in% installed.packages()[, "Package"])) {
    install.packages(pkg)
  }
}
```

]


---

# Multiple linear regression models

.font110[

Suppose that the yield in pounds of conversion in a chemical process depends on temperature and the catalyst concentration. A .darkorange[multiple linear regression] (MLR) model that might describe this relationship is $$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \epsilon_i, \quad i = 1, 2, \dots, n, \\ \epsilon_i \stackrel{iid}{\sim} \left(0, \sigma^2\right)$$

Hence, the mean response is $$E\left(Y_i\right) = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i}$$

.center.content-box-yellow[

This is an MLR model in two features: $X_1$ and $X_2$

]

]


---
class: clear

.code110[

```{r mlr-3d-df-01, fig.width=6, fig.asp=0.618, out.width="100%"}
# Simulate data from an MLR model
set.seed(101)  # for reproducibility
n <- 50
df <- tibble::tibble(
  x1 = runif(n),
  x2 = runif(n),
  y = 1 + 2*x1 - 3*x2 + rnorm(n, sd = 1)  #<<
)
head(df, n = 3)  # print first few rows
```

]


---

# Scatterplot matrices

.font125[

Can be useful to plot pairwise scatterplots:

* Base R: `pairs()`

* **ggplot2**: `GGally::ggpairs()`

* **lattice**: `lattice::splom()`

* **car**: `car::scatterplotMatrix()` or `car::spm()`

]

.code125[

```{r mlr-3d-df-02, eval=FALSE}
# CExample
pairs(df, cex = 1.2, pch = 19, 
      col = adjustcolor("darkred", alpha.f = 0.5))  #<<
```

]


---
class: clear, middle, center

```{r mlr-3d-df-02-01, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="100%"}
# Construct a scatterplot matrix
pairs(df, cex = 1.2, pch = 19, col = adjustcolor("darkred", alpha.f = 0.5))
```


---
class: clear, middle, center

.font300[

Interactive plots can be useful (for exploratory analysis) too!

https://plot.ly/r/

]


---
class: clear

.code120[

```{r mlr-3d-df-03, eval=FALSE}
library(plotly)  # for interactive plotting  #<<

# Draw (interactive) 3-D scatterplot
plot_ly(data = df, x = ~x1, y = ~x2, z = ~y, 
        mode = "markers", type = "scatter3d",
        marker = list(opacity = 0.7, symbol = 1, 
                      size = 5, color = "black")) %>%
  layout(
    scene = list(
      aspectmode = "manual", 
      aspectratio = list(x = 1, y = 1, z = 1),
      xaxis = list(title = "X1", range = c(0, 1)),
      yaxis = list(title = "X2", range = c(0, 1)),
      zaxis = list(title = "Y")
    )
  )
```

]


---
class: clear, middle, center

.scrollable[

```{r mlr-3d-df-04, echo=FALSE, out.width="100%"}
# Load required packages
library(plotly)  # for interactive plotting

# Draw (interactive) 3-D scatterplot
plot_ly(data = df, x = ~x1, y = ~x2, z = ~y, mode = "markers", 
        type = "scatter3d",
        marker = list(opacity = 0.7, symbol = 1, size = 5, color = "black")) %>%
  layout(
    scene = list(
      aspectmode = "manual", aspectratio = list(x = 1, y = 1, z = 1),
      xaxis = list(title = "X1", range = c(0, 1)),
      yaxis = list(title = "X2", range = c(0, 1)),
      zaxis = list(title = "Y")
    )
  )
```

]


---
class: clear

.scrollable[

```{r mlr-3d-df-05, eval=FALSE}
# Fit an MLR model to the simulated data
fit <- lm(y ~ x1 + x2, data = df)  #<<
(betas <- coef(fit))  #<<
## (Intercept)          x1          x2  #<<
##   0.8834363   2.3265433  -2.9942737  #<<

# Generate predictions over a fine grid  #<<
.x1 <- .x2 <- seq(from = 0, to = 1, length = 50)
yhat <- t(outer(.x1, .x2, function(x1, x2) {
  betas[1] + betas[2]*x1 + betas[3]*x2
}))

# Draw (interactive) 3-D scatterplot with fitted mean response
plot_ly(x = ~.x1, y = ~.x2, z = ~yhat, 
        type = "surface", opacity = 0.7) %>%
  add_trace(data = df, x = ~x1, y = ~x2, z = ~y, 
            mode = "markers", 
            type = "scatter3d",
            marker = list(opacity = 0.7, symbol = 1, 
                          size = 5, color = "black")) %>%
  layout(
    scene = list(
      aspectmode = "manual", 
      aspectratio = list(x = 1, y = 1, z = 1),
      xaxis = list(title = "X1", range = c(0, 1)),
      yaxis = list(title = "X2", range = c(0, 1)),
      zaxis = list(title = "Y")
    )
  )
```

]


---
class: clear, middle, center

.scrollable[

```{r mlr-3d-df-06, echo=FALSE, out.width="100%"}
# Fit an MLR model to the simulated data
fit <- lm(y ~ x1 + x2, data = df)
betas <- coef(fit)                

# Generate predictions over a fine grid  #<<
.x1 <- .x2 <- seq(from = 0, to = 1, length = 50)
yhat <- t(outer(.x1, .x2, function(x1, x2) {
  betas[1] + betas[2]*x1 + betas[3]*x2
}))

# Draw (interactive) 3-D scatterplot with fitted mean response
plot_ly(x = ~.x1, y = ~.x2, z = ~yhat, 
        type = "surface", opacity = 0.7) %>%
  add_trace(data = df, x = ~x1, y = ~x2, z = ~y, 
            mode = "markers", 
            type = "scatter3d",
            marker = list(opacity = 0.7, symbol = 1, 
                          size = 5, color = "black")) %>%
  layout(
    scene = list(
      aspectmode = "manual", 
      aspectratio = list(x = 1, y = 1, z = 1),
      xaxis = list(title = "X1", range = c(0, 1)),
      yaxis = list(title = "X2", range = c(0, 1)),
      zaxis = list(title = "Y")
    )
  )
```

]


---

# The general MLR model

.font125[

* Suppose we have a total of $k = p - 1$ predictors $X_1, X_2, \dots, X_{p-1}$. The MLR model is $$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \dots \beta_{p-1} X_{p-1} + \epsilon_i, \quad i = 1, 2, \dots, n$$ where we assume $\epsilon_i \stackrel{iid}{\sim} \left(0, \sigma^2\right)$

]

--

.font125[

* In short, we have a mean response of the form $$E\left(Y_i\right) = \beta_0 + \sum_{j = 1}^{p-1}\beta_j X_{ij} = \underbrace{\sum_{j = 0}^{p-1}\beta_j X_{ij}}_{\text{dot product}}, \quad \text{where } X_{i0} \equiv 1$$

]


---

# The general MLR model

.font125[

The MLR model with normal errors $$Y_i = \beta_0 + \sum_{j = 1}^{p-1}\beta_j X_{ij} + \epsilon_i, \quad i = 1, 2, \dots, n$$

]

.font125[

* $\beta_0, \beta_1, \dots, \beta_{p-1}$ are (unknown) regression coefficients (AKA weights or parameters)

* $X_{i1}, X_{i2}, \dots, X_{i,p-1}$ are known **constants** (i.e., assumed fixed by design)

* $\epsilon_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)$

]


---
class: clear, center, middle

```{r example-table, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/mlr-data.png")
```


---

# Categorical predictors

```{r dummy-encoding, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/dummy-encoding.png")
```

.center.font150[

More on categorical predictors in [Lecture 05](https://bgreenwell.github.io/uc-bana7052/slides/lecture-05#1)

]


---

# Estimating the coefficients

.font150[

The regression coefficients can be estimated using least squares by minimizing $$Q = \sum_{i = 1}^n\left(Y_i - \beta_0 - \beta_1 X_{i1} - \dots - \beta_{p-1} X_{i, p-1}\right)^2$$

]

.center.font150.content-box-yellow[

Equating the partial derivatives to zero amounts to solving a system of $n$ (linear) equations in $p$ unknowns

]


---
class: clear

background-image: url(images/matrix-approach.png)


---

# Matrix form of the MLR model

.font115[

 In matrix form, the MLR can be expressed as $\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$
 
 
 * $\boldsymbol{Y} = \left(Y_1, Y_2, \dots, Y_n\right)^\top$ is an $n \times 1$ vector of responses
 
 * $\boldsymbol{\beta} = \left(\beta_0, \beta_1, \dots, \beta_{p-1}\right)^\top$ is an $p \times 1$ vector of coefficients
 
 * $\boldsymbol{\epsilon} = \left(\epsilon_1, \epsilon_2, \dots, \epsilon_n\right)^\top \sim N\left(\boldsymbol{0}_n, \sigma^2 \boldsymbol{I}_n\right)$ .font125[.red[`r set.seed(2); emo::ji("point")` What does this mean? `r emo::ji("thinking")`]]
 
 * $\boldsymbol{X} = \begin{bmatrix} 1 & X_{11} & X_{12} & \cdots & X_{1,p-1} \\ 1 & X_{21} & X_{22} & \cdots & X_{2,p-1} \\ \vdots & \vdots & & \vdots \\ 1 & X_{n1} & X_{n2} & \cdots & X_{n,p-1} \\ \end{bmatrix}$ is an $n \times p$ **model matrix**

]


---

# Least squares estimation

.font150[

We want to find the value of $\boldsymbol{\beta}$ that minimizes $$\begin{align} \boldsymbol{Q} &= \left(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}\right)^\top\left(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}\right) \\ &= \boldsymbol{Y}^\top\boldsymbol{Y} - 2\boldsymbol{\beta}^\top\boldsymbol{X}^\top\boldsymbol{Y} + \boldsymbol{\beta}^\top\boldsymbol{X}^\top\boldsymbol{X}\boldsymbol{\beta} \end{align}$$

Differentiating $\boldsymbol{Q}$ w.r.t. $\boldsymbol{\beta}$ and equating to zero yields $$\frac{\partial \boldsymbol{Q}}{\partial \boldsymbol{\beta}} = -2\boldsymbol{X}^\top\boldsymbol{Y} + 2\boldsymbol{X}^\top\boldsymbol{X}\boldsymbol{\beta} = \boldsymbol{0}_p$$

* The .bold.red[normal equations]: $\boldsymbol{X}^\top\boldsymbol{X}\boldsymbol{\beta} = \boldsymbol{X}^\top\boldsymbol{Y}$

]


---

# Least squares estimation

.font200[

<br>

Solving the normal equations for $\boldsymbol{\beta}$ leads to the least squares estimate:

$$\widehat{\boldsymbol{\beta}} = \left(\boldsymbol{X}^\top\boldsymbol{X}\right)^{-1}\boldsymbol{X}^\top\boldsymbol{Y}$$

]


---
class: clear, middle, center

.font200[

$\boldsymbol{X}$ has to be .content-box-red[full rank] in order for $\left(\boldsymbol{X}^\top\boldsymbol{X}\right)$ to be invertible!

]

--

.center[.font125[.content-box-yellow[

This is one reason why we dummy encode categorical variables

]]]

---

# The fitted model

.font150[

* Fitted values: $\widehat{\boldsymbol{Y}} = \boldsymbol{X}\widehat{\boldsymbol{\beta}} = \boldsymbol{X}\left(\boldsymbol{X}^\top\boldsymbol{X}\right)^{-1}\boldsymbol{X}^\top\boldsymbol{Y} = \boldsymbol{H}\boldsymbol{Y}$

]

.font150[

* Residuals: $\boldsymbol{\epsilon} = \boldsymbol{Y} - \widehat{\boldsymbol{Y}} = \boldsymbol{Y} - \boldsymbol{X}\widehat{\boldsymbol{\beta}} = \left(\boldsymbol{I} - \boldsymbol{H}\right)\boldsymbol{Y}$

]

.font150.content-box-yellow[

$\boldsymbol{H}$ is both *symmetric* (i.e., $\boldsymbol{H}^\top = \boldsymbol{H}$) and *idempotent* (i.e., $\boldsymbol{H} = \boldsymbol{H}\boldsymbol{H}$) and is referred to as the *hat matrix* (the diagonal entries of $H$ are important in .bold.red[detecting "influential" observations])

]


---

# Properties of $\widehat{\boldsymbol{\beta}}$

.font150[

Assuming $\boldsymbol{\epsilon} \sim N\left(\boldsymbol{0}_n, \sigma^2\boldsymbol{I}_n\right)$, what are some properties of the LS estimate of $\boldsymbol{\beta}$?

]

.font150[

* Unbiased: $E\left(\widehat{\boldsymbol{\beta}}\right) = \boldsymbol{\beta}$

* Variance-covariance matrix: $Var\left(\widehat{\boldsymbol{\beta}}\right) = \sigma^2 \left(\boldsymbol{X}^\top\boldsymbol{X}\right)^{-1}$

* Sampling distribution: $\widehat{\boldsymbol{\beta}} \sim N\left(\boldsymbol{\beta}, \sigma^2 \left(\boldsymbol{X}^\top\boldsymbol{X}\right)^{-1}\right)$

]


---

# Delivery data example `r anicon::faa("truck", animate = "passing", speed = "slow")`

.font110[

A soft drink bottler is analyzing vending machine service routes in their distribution system. They are interested in predicting the amount of time required by the route driver to service the vending machines in an outlet. This service activity includes stocking the machine with beverage products and minor maintenance or housekeeping. The industrial engineer responsible for the study has suggested that the two most important variables affecting delivery time in minutes ( $Y$ ) are the number of cases of product stocked ( $X_1$ ) and the distance walked by the route driver in feet ( $X_2$ ). The engineer has collected $n = 25$ observations on delivery time which are stored in the file [delivery.csv](https://bgreenwell.github.io/uc-bana7052/data/delivery.csv). We'll use these data to fit the MLR model $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$.

]


---

# Delivery data example `r anicon::faa("truck", animate = "passing", speed = "slow")`

.font125[

```{r delivery-01}
# Load the delivery data
url <- "https://bgreenwell.github.io/uc-bana7052/data/delivery.csv"
delivery <- read.csv(url)
head(delivery, n = 5)  # print first 5 observations
```

]


---
class: clear, middle

.pull-left[

.code125[

```{r delivery-ggpairs-01, eval=FALSE}
GGally::ggpairs(
  data = delivery[, -1]  #<<
)  
```

]

]

.pull-right[

```{r delivery-ggpairs-02, echo=FALSE, fig.width=5, fig.height=5, out.width="100%"}
GGally::ggpairs(
  data = delivery[, -1]  #<<
)  
```

]


---
class: clear, middle

.pull-left[

.code125[

```{r delivery-splom-01, eval=FALSE}
lattice::splom(
  x = delivery[, -1],  #<<
  type = c("p", "smooth"), 
  pch = 19, 
  col = "dodgerblue2", 
  lty = "dotted", 
  alpha = 0.6
)
```

]

]

.pull-right[

```{r delivery-splom-02, echo=FALSE, fig.width=5, fig.height=5, out.width="100%"}
lattice::splom(
  x = delivery[, -1],  #<<
  type = c("p", "smooth"), 
  pch = 19, 
  col = "dodgerblue2", 
  lty = "dotted", 
  alpha = 0.6
)
```

]


---

# Delivery data example `r anicon::faa("truck", animate = "passing", speed = "slow")`

.code125[

```{r delivery-lm-01}
# Fit a multile linear regression model
delivery_fit <- lm(DeliveryTime ~ NumberofCases + 
                     Distance, data = delivery)

# Extract estimated coefficients
coef(delivery_fit)  
```

]


---

# Delivery data example `r anicon::faa("truck", animate = "passing", speed = "slow")`

.code125[

```{r delivery-lm-02}
# Fit a multile linear regression model
delivery_fit <- lm(DeliveryTime ~ ., 
                   data = delivery)

# Extract estimated coefficients
coef(delivery_fit)  
```

]

.font150.center[

`y ~ ., data = df` is shorthand for regress `y` on every other column in `df`

]


---

# Delivery data example `r anicon::faa("truck", animate = "passing", speed = "slow")`

.code125[

```{r delivery-lm-03}
# Fit a multile linear regression model
delivery <- subset(delivery, select = -Index)  #<<
(delivery_fit <- lm(DeliveryTime ~ ., data = delivery))
```

]


---

# How do we interpret $\widehat{\beta}_i$?

.code125[

```{r delivery-lm-04}
round(coef(delivery_fit), digits = 3)  #<<
```

]

--

.font150[

* .bold[.red[All else held constant]], for every one additional case, the mean delivery time increases by 1.616 minutes

* .bold[.red[All else held constant]], for every one-unit increase in distance, the mean delivery time increases by 0.014 minutes

]


---
class: clear, middle, center

.font150.center.content-box-red[

In general, $\widehat{\beta}_j$ is the estimated increase in the mean response per one-unit increase in $X_j$ (**all else held constant**)

]


---

# Delivery data example `r anicon::faa("truck", animate = "passing", speed = "slow")`

.scrollable[

.font150[

```{r delivery-matrix-wrong}
xnames <- c("NumberofCases", "Distance")
X <- data.matrix(delivery[, xnames])
head(X)
Y <- delivery$DeliveryTime
solve(t(X) %*% X) %*% t(X) %*% Y  #<<
```

]

]


---

# Delivery data example `r anicon::faa("truck", animate = "passing", speed = "slow")`

.scrollable[

.font150[

```{r delivery-matrix-right}
X <- model.matrix(~ NumberofCases + Distance, 
                  data = delivery)
head(X)
Y <- delivery$DeliveryTime
solve(t(X) %*% X) %*% t(X) %*% Y  #<<
```

]

]


---

# Delivery data example `r anicon::faa("truck", animate = "passing", speed = "slow")`

.font125[

```{r delivery-output}
# Extract fitted values and residuals
.fitted <- fitted(delivery_fit)
.resids <- residuals(delivery_fit)
head(cbind(delivery, .fitted, .resids))
```

]

--

.font125.center[

What would `.fitted + .resids` produce?

]


---

# Your turn

.font125[

Harrison and Rubinfeld (1978) were among the first to analyze the well-known Boston housing data. One of their goals was to find a [housing value equation](https://bgreenwell.github.io/pdp/articles/pdp.html) using data on median home values from $n =506$ census tracts in the suburbs of Boston from the 1970 census; see `?pdp::boston` for a description of each variable. Fit an MLR model using `cmed` as the response and `lstat` and `rm` as the predictors and interpret the coefficients. Be sure to construct a scatterplot matrix as well. What do you predict the median value to be for a census tract with `lstat` = 20 and `rm` = 6? **Hint:** to load the data, use the following code chunk:

```{r boston-load, eval=FALSE}
data(boston, package = "pdp")
```

]


---
class: clear, middle

```{r boston-spm-01, fig.width=6, fig.asp=0.618, out.width="100%"}
pairs(pdp::boston[, c("cmedv", "lstat", "rm")], pch = 19,
      col = "black")
```

---
class: clear, middle

```{r boston-spm-02, fig.width=6, fig.asp=0.618, out.width="100%"}
pairs(pdp::boston[, c("cmedv", "lstat", "rm")], pch = 19, 
      col = adjustcolor("black", alpha.f = 0.2))  #<<
```


---

# Solution

.code125[

```{r boston-mlr}
coef(boston_fit <- lm(cmedv ~ lstat + rm, 
                      data = pdp::boston))
```

]

--

.font150[

All else held constant, within a census tract, we estimate that the average median home value will .red.bold[decrease] by `r scales::dollar(abs(coef(lm(cmedv ~ lstat + rm, data = pdp::boston))[2]*1000))` for every 1% increase in `lstat`

]

---

# Solution

.scrollable[

.code125[

```{r boston-predict}
predict(
  object = boston_fit, 
  newdata = data.frame(lstat = 20, rm = 6),  #<<
  se.fit = TRUE,  #<<
  interval = "confidence"  #<<
)
```

]

]


---

# Inferences in the MLR model

.font160[

Once we fit an MLR model, we might ask ourselves:

* How well does the model fit the data? (Lectures [04](https://bgreenwell.github.io/uc-bana7052/slides/lecture-04#1)/[06](https://bgreenwell.github.io/uc-bana7052/slides/lecture-06#1))
    
* Which predictors seem "important"?

]

--

.font125.center.content-box-green[

Statistical inference in the MLR model is not that different from inference in SLR (only a slight change in the formulas)

]


---

# Significance of the regression

.font150.center.content-box-purple[

Is there a (statistically significant) linear relationship between the response and **ANY** of the features?

]

--

.font150[

Hypotheses:

$\quad H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0$

$\quad H_1: \beta_j \ne 0$ for at least one $j$

]


---

# The general linear test

.font150[

.bold[Full model]: $Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_k X_{i, p-1} + \epsilon_i$

.bold[Reduced model]: $Y_i = \beta_0 + \epsilon_i$ (if $H_0$ were true)

]

--

.font150[


* $F_{obs} = \frac{SSE(R) - SSE(F)}{df_R - df_F} \div \frac{SSE(F)}{df_F} = \frac{MSR}{MSE}$
    
* Reject $H_0$ whenever $F_{obs} > F_{1 - \alpha, df_R - df_F, df_F}$
    
    - Here $df_R - df_F = p - 1$ and $df_F = n - p$
    
]


---
class: clear

```{r delivery-ftest-01, highlight.output=c(12:13, 19)}
summary(delivery_fit)
```


---

# Delivery data example `r anicon::faa("truck", animate = "passing", speed = "slow")`

.font125[

```{r delivery-ftest-02, highlight.output=7}
# Manually construct F-test
delivery_fit_reduced <- lm(DeliveryTime ~ 1, data = delivery)
anova(delivery_fit_reduced, delivery_fit)
```

]


---

# Coefficient of determination

.pull-left[

.font200[R-squared]

.font125[

* $R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$

* $R^2$ will always increase as more terms are added to the model!

.center[`r anicon::cia("https://emojis.slackmojis.com/emojis/images/1471045885/967/wtf.gif?1471045885", animate = FALSE, size = 6)`]

]

]

--

.pull-right[

.font200[Adjusted R-squared]

.font125[

* $\begin{align} R_{adj}^2 &= 1 - \frac{SSR / \left(n - p\right)}{SST / \left(n - 1\right)} \\ &= 1 - \left(\frac{n - 1}{n - p}\right)\frac{SSR}{SST} \end{align}$
    
* Penalizes $R^2$ if there are "too many" terms in the model

]

]

---
class: clear, middle

```{r delivery-rsquared-01, highlight.output=18}
summary(delivery_fit)
```


---

# Delivery data example `r anicon::faa("truck", animate = "passing", speed = "slow")`

.scrollable[

.font125[

```{r delivery-rsquared-02, highlight.output=c(14:16, 21)}
# Simulate new columns at random
set.seed(101)  # for reproducibility
delivery2 <- delivery
delivery2$X3 <- rnorm(nrow(delivery))
delivery2$X4 <- rnorm(nrow(delivery))
delivery2$X5 <- rnorm(nrow(delivery))

# Update the fitted mode
delivery2_fit <- lm(DeliveryTime ~ ., data = delivery2)

# Print model summary
summary(delivery2_fit)
```

]

]


---

# Your turn

.font150[

Fit an MLR to the Boston housing data using all of the available predictors. Is the regression significant? Use the general *F*-test to determine whether the model with only the predictors `lstat` and `rm` is adequate.

]


---

# Solution

```{r boston-ftest, highlight.output=9}
# General F-test
data(boston, package = "pdp")  # Load the data
fit1 <- lm(cmedv ~ ., data = boston)  # full model
fit2 <- lm(cmedv ~ . - lstat - rm, data = boston)  # reduced model
anova(fit2, fit1)  # reduced model goes first  #<<
```


---

# Inference for $\beta_j$

.font150[

Hypothesis test for a **single coefficient** (this is called a *marginal test*):

$$H_0: \beta_j = 0 \quad vs. \quad H_1: \beta_j \ne 0$$

* Test statistic: $t_{obs} = \widehat{\beta}_j / \widehat{SE}\left(\widehat{\beta}_j\right)$

* Reject $H_0$ whenever $\left|t_{obs}\right| > t_{1 - \alpha/2, n - p}$

* $\left(1 - \alpha\right)100$% CI for $\beta_j$: $\widehat{\beta}_j \pm t_{1 - \alpha/2, n - p} \widehat{SE}\left(\widehat{\beta}_j\right)$

]


---

# Delivery data example `r anicon::faa("truck", animate = "passing", speed = "slow")`

.scrollable[

.font125[

```{r delivery-inference}
# Print summary of the model
summary(delivery_fit)  # SEs and marginal tests  #<<

# Construct 95% CIs for the coefficients
confint(delivery_fit, level = 0.95)
```

]

]


---
class: clear, center, middle

.font200[

[Polynomial regression](https://github.com/bgreenwell/uc-bana7052/blob/master/code/hardwood.R)

]


---

# Polynomial regression

.font130[

* Just a special case of the MLR model

* A second order model in a single predictor $$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon$$

* A *k*-th order model in a single predictor $$Y = \beta_0 + \sum_{j=1}^k\beta_j X^j + \epsilon$$ 

    - Typically $k \le 3$

]


---

# Some cautions `r emo::ji("warning")`

.pull-left[

* Keep the order of the model as low as possible

    - This is especially true if you are using the model as a predictor ([*over fitting*](https://bgreenwell.github.io/intro-ml-r/intro-ml-r.html#14))

    - Use the simplest model possible to explain the data, but no simpler (*parsimony*)

    - An $n - 1$ order model can perfectly fit a data set with $n$ observations (Why is this bad `r emo::ji("thinking")`)

]

.pull-right[

* Two model-building strategies:

    1. Fit the lowest order polynomial possible and build up (forward selection)
    
    2. Fit the highest order polynomial of interest, and remove terms one at a time (backward elimination)
    
    - These two procedures may not result in the same final model

* Increasing the order can result in an ill-conditioned $\boldsymbol{X}^\top\boldsymbol{X}$ and *multicollinearity* `r set.seed(102); emo::ji("sick")`

]


---
class: clear, center, middle

.font140[

Later in the course, if time permits, we'll cover [*multivariate adaptive regression splines*](https://koalaverse.github.io/AnalyticsSummit18/04-MARS.html#1) (MARS), an automatic multiple linear regression procedure

]

```{r hell-yeah, echo=FALSE, out.width="40%"}
RBitmoji::plot_comic("8b06e67b-d4e9-4f11-a355-f1236df17079-v1", tag = "hell yeah")
```


---
class: clear, middle, center

```{r quittin-time, echo=FALSE, out.width="60%"}
RBitmoji::plot_comic(my_id, tag = "quittin")
```
