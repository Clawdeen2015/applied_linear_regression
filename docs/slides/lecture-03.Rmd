---
title: "Multiple Linear Regression"
subtitle: "Lecture 03"
author: "Brandon M. Greenwell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: [default, metropolis, metropolis-fonts, hygge, "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: clear 

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)

# Global chunk options
knitr::opts_chunk$set(
  cache = FALSE,
  echo = TRUE,
  dev = "svglite",
  fig.align = "center",
  # fig.width = 6,
  # fig.asp = 0.618,
  # out.width = "70%",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

# Load required packages
library(dplyr)
library(patchwork)
```

background-image: url(images/linear-algebra.jpg)


---
# Reading assignment

.larger[

* Chapters: 5

    - Sections: 5.9-5.11; 6.1-6.4

* Main topics:

    - Regression in matrix form (5.9-5.11)
   
    - Multiple linear regression (6.1-6.4)

  
]


---

# Prerquisites

.scrollable[

```{r prerequisites, eval=FALSE}
# List of required (CRAN) packages
pkgs <- c(
  "pdp",        # for (corrected) Boston housing data
  "plotly",     # for interactive plots
  "tibble"      # for nicer data frames
)

# Install required (CRAN) packages
for (pkg in pkgs) {
  if (!requireNamespace(pkg)) {  # check if already installed first
    install.packages(pkg)  # install it
  }
}

# Install additional (optional) awesomeness
install.packages(c("devtools", "magick"))
devtools::install_github("bgreenwell/roundhouse")
```

]


---

# Ready to begin?

--

```{r roundhouse-01}
roundhouse::kick("When someone tells you to memorize the formulas in SLR", 
                 type = 2, fps = 10, width = 50)
```


---

# Multiple linear regression models

Suppose that the yield in pounds of conversion in a chemical process depends on temperature and the catalyst concentration. A .darkorange[multiple linear regression] (MLR) model that might describe this relationship is $$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \epsilon_i, \quad i = 1, 2, \dots, n$$
where $$\epsilon_i \stackrel{iid}{\sim} \left(0, \sigma^2\right)$$

--

Hence, the mean response is $$E\left(Y_i\right) = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i}$$

--

.center[.content-box-yellow[

This is an MLR model in two features $X_1$ and $X_2$

]]


---
class: clear

.scrollable[

```{r mlr-3d-sim-01, fig.width=6, fig.asp=0.618, out.width="100%"}
# Load required packages
library(plotly)
library(tibble)

# Simulate data from an MLR model
set.seed(101)  # for reproducibility
n <- 50
sim <- tibble(
  x1 = runif(n),
  x2 = runif(n),
  y = 1 + 2*x1 - 3*x2 + rnorm(n, sd = 1)  #<<
)

# Print first few observations
print(sim)
```

]


---
class: clear

.scrollable[

```{r mlr-3d-sim-02, fig.width=6, fig.asp=0.618, out.width="90%"}
# Construct a scatterplot matrix
pairs(sim, cex = 1.2, pch = 19, col = adjustcolor("purple", alpha.f = 0.5))
```

]


---
class: clear

.medium[

```{r mlr-3d-sim-03, eval=FALSE}
# Load required packages
library(plotly)  # for interactive plotting  #<<

# Draw (interactive) 3-D scatterplot
plot_ly(data = sim, x = ~x1, y = ~x2, z = ~y, mode = "markers", 
        type = "scatter3d",
        marker = list(opacity = 0.7, symbol = 1, 
                      size = 5, color = "black")) %>%
  layout(
    scene = list(
      aspectmode = "manual", 
      aspectratio = list(x = 1, y = 1, z = 1),
      xaxis = list(title = "X1", range = c(0, 1)),
      yaxis = list(title = "X2", range = c(0, 1)),
      zaxis = list(title = "Y")
    )
  )
```

]


---
class: clear, middle, center

.scrollable[

```{r mlr-3d-sim-04, echo=FALSE, out.width="100%"}
# Load required packages
library(plotly)  # for interactive plotting

# Draw (interactive) 3-D scatterplot
plot_ly(data = sim, x = ~x1, y = ~x2, z = ~y, mode = "markers", 
        type = "scatter3d",
        marker = list(opacity = 0.7, symbol = 1, size = 5, color = "black")) %>%
  layout(
    scene = list(
      aspectmode = "manual", aspectratio = list(x = 1, y = 1, z = 1),
      xaxis = list(title = "X1", range = c(0, 1)),
      yaxis = list(title = "X2", range = c(0, 1)),
      zaxis = list(title = "Y")
    )
  )
```

]


---
class: clear

.scrollable[

```{r mlr-3d-sim-05, eval=FALSE}
# Fit an MLR model to the simulated data
fit <- lm(y ~ x1 + x2, data = sim)  #<<
(betas <- coef(fit))  #<<
## (Intercept)          x1          x2  #<<
##   0.8834363   2.3265433  -2.9942737  #<<

# Generate predictions over a fine grid  #<<
.x1 <- .x2 <- seq(from = 0, to = 1, length = 50)
yhat <- t(outer(.x1, .x2, function(x1, x2) {
  betas[1] + betas[2]*x1 + betas[3]*x2
}))

# Draw (interactive) 3-D scatterplot with fitted mean response
plot_ly(x = ~.x1, y = ~.x2, z = ~yhat, 
        type = "surface", opacity = 0.7) %>%
  add_trace(data = sim, x = ~x1, y = ~x2, z = ~y, 
            mode = "markers", 
            type = "scatter3d",
            marker = list(opacity = 0.7, symbol = 1, 
                          size = 5, color = "black")) %>%
  layout(
    scene = list(
      aspectmode = "manual", 
      aspectratio = list(x = 1, y = 1, z = 1),
      xaxis = list(title = "X1", range = c(0, 1)),
      yaxis = list(title = "X2", range = c(0, 1)),
      zaxis = list(title = "Y")
    )
  )
```

]


---
class: clear, middle, center

.scrollable[

```{r mlr-3d-sim-06, echo=FALSE, out.width="100%"}
# Fit an MLR model to the simulated data
fit <- lm(y ~ x1 + x2, data = sim)
betas <- coef(fit)                

# Generate predictions over a fine grid  #<<
.x1 <- .x2 <- seq(from = 0, to = 1, length = 50)
yhat <- t(outer(.x1, .x2, function(x1, x2) {
  betas[1] + betas[2]*x1 + betas[3]*x2
}))

# Draw (interactive) 3-D scatterplot with fitted mean response
plot_ly(x = ~.x1, y = ~.x2, z = ~yhat, 
        type = "surface", opacity = 0.7) %>%
  add_trace(data = sim, x = ~x1, y = ~x2, z = ~y, 
            mode = "markers", 
            type = "scatter3d",
            marker = list(opacity = 0.7, symbol = 1, 
                          size = 5, color = "black")) %>%
  layout(
    scene = list(
      aspectmode = "manual", 
      aspectratio = list(x = 1, y = 1, z = 1),
      xaxis = list(title = "X1", range = c(0, 1)),
      yaxis = list(title = "X2", range = c(0, 1)),
      zaxis = list(title = "Y")
    )
  )
```

]


---

# The general MLR model

.large[

* Suppose we have a total of $k = p - 1$ predictors $X_1, X_2, \dots, X_{p-1}$. The MLR model is $$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \dots \beta_{p-1} X_{p-1} + \epsilon_i, \quad i = 1, 2, \dots, n$$ where we assume $\epsilon_i \stackrel{iid}{\sim} \left(0, \sigma^2\right)$

]

--

.large[

* In short, we have a mean response of the form $$E\left(Y_i\right) = \beta_0 + \sum_{j = 1}^{p-1}\beta_j X_{ij} = \underbrace{\sum_{j = 0}^{p-1}\beta_j X_{ij}}_{\text{dot product}}, \quad \text{where } X_{i0} \equiv 1$$

]


---

# The general MLR model

The MLR model with normal errors $$Y_i = \beta_0 + \sum_{j = 1}^{p-1}\beta_j X_{ij} + \epsilon_i, \quad i = 1, 2, \dots, n$$ where

--

* $\beta_0, \beta_1, \dots, \beta_{p-1}$ are (unknown) regression coefficients (AKA weights or parameters)

* $X_{i1}, X_{i2}, \dots, X_{i,p-1}$ are known constants

* $\epsilon_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)$

* $i = 1, 2, \dots, n$


---
class: clear, center, middle

```{r example-table, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/mlr-data.png")
```


---

# Categorical predictors

* Also called .magenta[*qualitative*] predictors or .magenta[*factors*]

    - [Free DataCamp exercise](https://campus.datacamp.com/courses/free-introduction-to-r/chapter-4-factors-4?ex=4)

--

* [Wikipedia states that](https://en.wikipedia.org/wiki/Categorical_variable) "In statistics, a categorical variable is a variable that can take on one of a limited, and usually fixed number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property."

    - Examples include gender (i.e., male/female), .red[**zip code**], political affiliation, etc.

* In regression, we typically use *indicator variables* that take on the values 0 and 1 to identify the classes of a categorical variable

---
class: clear, center, middle

```{r dummy-encoding, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/dummy-encoding.png")
```


---

# Categorical variables in R

.scrollable[

```{r categorical-variables}
# Categorical variable
(dow <- c("Mon", "Tue", "Wed", "Thu", "Fri"))
class(dow)

# Coerce to a factor (needed for some functions, like lm())
(dow2 <- as.factor(dow))
class(dow2)
```

]


---

# Estimating the coefficients

.center[.bold[.purple[.larger[
 How can we estimate $\beta_0, \beta_1, \dots, \beta_{p-1}$?
]]]]

--

.large[

The regression coefficients can easily be obtained using the LS approach; that is, finding the values of the regression coefficients that minimize $$Q = \sum_{i = 1}^n\left(Y_i - \beta_0 - \beta_1 X_{i1} - \dots - \beta_{p-1} X_{i, p-1}\right)^2$$

]

--

.center[.small[.content-box-yellow[Equating the partial derivatives to zero amounts to solving a system of *n* equations in *p* unknowns]]]


---
class: clear

background-image: url(images/matrix-approach.png)


---

# Matrix form of the MLR model

 In matrix form, the MLR can be expressed as $$\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$$ where 
 
 * $\boldsymbol{Y} = \left(Y_1, Y_2, \dots, Y_n\right)^\top$ is an $n \times 1$ vector of responses
 
 * $\boldsymbol{\beta} = \left(\beta_0, \beta_1, \dots, \beta_{p-1}\right)^\top$ is an $p \times 1$ vector of coefficients
 
 * $\boldsymbol{\epsilon} = \left(\epsilon_1, \epsilon_2, \dots, \epsilon_n\right)^\top \sim N\left(\boldsymbol{0}_n, \sigma^2 \boldsymbol{I}_n\right)$ .red[`r emo::ji("left")` What does this mean? `r emo::ji("thinking")`] 
 
 * $\boldsymbol{X} = \begin{bmatrix} X_{11} & X_{12} & \cdots & X_{1,p-1} \\ X_{21} & X_{22} & \cdots & X_{2,p-1} \\ \vdots & \vdots & & \vdots \\ X_{n1} & X_{n2} & \cdots & X_{n,p-1} \\ \end{bmatrix}$ is an $n \times p$ .purple[*model matrix* (or *design matrix*)]


---

# Least squares estimation

We want to find the value of $\boldsymbol{\beta}$ that minimizes $$\boldsymbol{Q} = \left(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}\right)^\top\left(\boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}\right)$$ which simplifies to $$\boldsymbol{Q} = \boldsymbol{Y}^\top\boldsymbol{Y} - 2\boldsymbol{\beta}^\top\boldsymbol{X}^\top\boldsymbol{Y} + \boldsymbol{\beta}^\top\boldsymbol{X}^\top\boldsymbol{X}\boldsymbol{Y}$$

--

Differentiating $\boldsymbol{Q}$ w.r.t. $\boldsymbol{\beta}$ and equating to zero yields $$\frac{\partial \boldsymbol{Q}}{\partial \boldsymbol{\beta}} = -2\boldsymbol{X}^\top\boldsymbol{Y} + 2\boldsymbol{X}^\top\boldsymbol{X}\boldsymbol{\beta} = \boldsymbol{0}_p$$

--

* The normal equations: $\boldsymbol{X}^\top\boldsymbol{X}\boldsymbol{\beta} = \boldsymbol{X}^\top\boldsymbol{Y}$

.center[.content-box-yellow[

$$\widehat{\boldsymbol{\beta}} = \left(\boldsymbol{X}^\top\boldsymbol{X}\right)^{-1}\boldsymbol{X}^\top\boldsymbol{Y}$$

]]


---
class: clear, middle, center

.larger[

$\boldsymbol{X}$ has to be full rank in order for $\left(\boldsymbol{X}^\top\boldsymbol{X}\right)$ to be invertible!

]


---

# The fitted model

.large[

* Fitted values: $\widehat{\boldsymbol{Y}} = \boldsymbol{X}\widehat{\boldsymbol{\beta}} = \boldsymbol{X}\left(\boldsymbol{X}^\top\boldsymbol{X}\right)^{-1}\boldsymbol{X}^\top\boldsymbol{Y} = \boldsymbol{H}\boldsymbol{Y}$

]

--

.large[

* Residuals: $\boldsymbol{\epsilon} = \boldsymbol{Y} - \widehat{\boldsymbol{Y}} = \boldsymbol{Y} - \boldsymbol{X}\widehat{\boldsymbol{\beta}} = \left(\boldsymbol{I} - \boldsymbol{H}\right)\boldsymbol{X}$

]

--

.large[.center[.content-box-yellow[

$\boldsymbol{H}$ is both *symmetric* (i.e., $\boldsymbol{H}^\top = \boldsymbol{H}$) and *idempotent* (i.e., $\boldsymbol{H} = \boldsymbol{H}\boldsymbol{H}$) and is referred to as the *hat matrix* (the diagonal entries of $H$ are important in .red[detecting "influential" observations])

]]]


---

# Properties of $\widehat{\boldsymbol{\beta}}$

Assuming $\boldsymbol{\epsilon} \sim N\left(\boldsymbol{0}_n, \sigma^2\boldsymbol{I}_n\right)$, what are some properties of the LS estimate of $\boldsymbol{\beta}$?

--

* Unbiased: $E\left(\widehat{\boldsymbol{\beta}}\right) = \boldsymbol{\beta}$

* Variance-covariance matrix: $Var\left(\widehat{\boldsymbol{\beta}}\right) = \sigma^2 \left(\boldsymbol{X}^\top\boldsymbol{X}\right)^{-1}$

* Sampling distirbution: $\widehat{\boldsymbol{\beta}} \sim N\left(\boldsymbol{\beta}, \sigma^2 \left(\boldsymbol{X}^\top\boldsymbol{X}\right)^{-1}\right)$


---

# Delivery data example

```{r delivery-01}
# Load the delivery data
url <- "https://bgreenwell.github.io/uc-bana7052/data/delivery.csv"
delivery <- read.csv(url)
head(delivery, n = 5)  # print first 5 observations
```


---

# Delivery data example

```{r delivery-pairs, fig.width=5, fig.height=5, out.width="70%"}
# Scatterplot matrix
pairs(delivery[, -1], pch = 19, col = adjustcolor("purple", alpha.f = 0.6))
```


---

# Delivery data example

```{r delivery-splom, fig.width=6, fig.height=6, out.width="70%"}
# Another scatterplot matrix
lattice::splom(delivery[, -1], type = c("p", "smooth"), pch = 19, 
               col = 1, lty = "dotted", alpha = 0.6)
```


---

# Delivery data example

.large[

```{r delivery-lm-01}
# Fit a linear model
(fit <- lm(DeliveryTime ~ NumberofCases + Distance, data = delivery))
```

]


---

# Delivery data example

.large[

```{r delivery-lm-02}
# Fit a linear model
(fit <- lm(DeliveryTime ~ ., data = delivery))
```

]

.medium[.center[.content-box-red[

`y ~ ., data = df` is shorthand for regress `y` on every other column in `df`

]]]


---

# Delivery data example

.large[

```{r delivery-lm-03}
# Fit a linear model
delivery <- subset(delivery, select = -Index)  #<<
(fit <- lm(DeliveryTime ~ ., data = delivery))
```

]


---

# How do we interpret $\widehat{\beta}_i$?

.medium[

```{r delivery-lm-04}
round(coef(fit), digits = 3)  #<<
```

]

--

.medium[

* .bold[.red[All else held constant]], for every one additional case, the mean delivery time increases by 1.616 minutes

]

--

.medium[

* .bold[.red[All else held constant]], for every one-unit increase in distance, the mean delivery time increases by 0.014 minutes

]

--

.medium[.center[.content-box-green[

In general, $\widehat{\beta}_j$ is the estimated increase in the mean repsonse per one-unit increase in $X_j$ (all else held constant)

]]]


---
# Delivery data example

.scrollable[

.large[

```{r delivery-matrix-wrong}
xnames <- c("NumberofCases", "Distance")
X <- data.matrix(delivery[, xnames])
head(X)
Y <- delivery$DeliveryTime
solve(t(X) %*% X) %*% t(X) %*% Y  #<<
```

]

]


---
# Delivery data example

.scrollable[

.large[

```{r delivery-matrix-right}
X <- model.matrix(~ NumberofCases + Distance, 
                  data = delivery)
head(X)
Y <- delivery$DeliveryTime
solve(t(X) %*% X) %*% t(X) %*% Y  #<<
```

]

]


---

# Delivery data example

.medium[

```{r delivery-output}
# Extract fitted values and residuals
.fitted <- fitted(fit)
.resids <- residuals(fit)
head(cbind(delivery, .fitted, .resids))
```

]

--

.medium[.center[.content-box-green[

What would `.fitted + .resids` produce?

]]]


---

# Your turn `r emo::ji("poop")`

.medium[

Harrison and Rubinfeld (1978) were among the first to analyze the well-known Boston housing data. One of their goals was to find a housing value equation using data on median home values from $n =506$ census tracts in the suburbs of Boston from the 1970 census; see `?pdp::boston` for a description of each variable. Fit an MLR model using `cmed` as the response and `lstat` and `rm` as the predictors and interpret the coefficients. Be sure to construct a scatterplot matrix as well. What do you predict the median value to be for a census tract with `lstat` = 20 and `rm` = 6? **Hint:** to load the data, use the following code chunk:

```{r boston-load, eval=FALSE}
data(boston, package = "pdp")
```

]


---

# Solution `r emo::ji("sunglasses")`

```{r boston-spm-01, fig.width=6, fig.asp=0.618, out.width="80%"}
pairs(pdp::boston[, c("cmedv", "lstat", "rm")], pch = 19,
      col = "black")
```

---


# Solution `r emo::ji("sunglasses")`

```{r boston-spm-02, fig.width=6, fig.asp=0.618, out.width="80%"}
pairs(pdp::boston[, c("cmedv", "lstat", "rm")], pch = 19, 
      col = adjustcolor("black", alpha.f = 0.2))  #<<
```


---

# Solution `r emo::ji("sunglasses")`

.large[

```{r boston-mlr}
coef(boston_fit <- lm(cmedv ~ lstat + rm, 
                      data = pdp::boston))
```

]

--

.larger[

* All else held constant, within a census tract, we expect median home value to .red.bold[decrease] by `r scales::dollar(abs(coef(lm(cmedv ~ lstat + rm, data = pdp::boston))[2]*1000))` for every 1% increase in `lstat`

]

---

# Solution `r emo::ji("sunglasses")`

.scrollable[

.medium[

```{r boston-predict}
predict(
  object = boston_fit, 
  newdata = data.frame(lstat = 20, rm = 6),  #<<
  se.fit = TRUE,  #<<
  interval = "confidence"  #<<
)
```

]

]


---

# Interaction effects

* Lecture 05

    - Give small introduction here and introduce polynomial regression as a special case


---

# Polynomial regression

TBD
