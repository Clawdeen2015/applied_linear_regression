---
title: 'BANA 7052: Applied Linear Regression'
subtitle: Final exam (60 points)
output:
  html_document: default
  word_document: default
---

**Instructions:** The final is take home and open book. It will be due by noon on Friday, December 7-th. Please upload your competed final to blackboard. This is **NOT** a group assignment and absolutely **NO** working together. Failure to do so will result in a zero for the final. There is a bonus question at the end worth five points.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Part I: multiple choice

**Please clearly circle your multiple choice answers**

**Question 1:** In a multiple linear regression model, which of the following are considered as random?

a. The continuous response or dependent variable $\boldsymbol{Y}$
b. The expected value of $\boldsymbol{Y}$
c. The unknown parameter $\boldsymbol{\beta}$
d. The unknown error variance $\sigma^2$


**Question 2:** When developing a linear regression model, adding additional predictors to the model will

a. Always increase $R^2$
b. Always increase adjusted $R^2$
c. Sometimes decrease $R^2$
d. Always decrease adjusted $R^2$


**Question 3:** What is the best way to identify multicollinearity?

a. Including interaction terms in the linear regression model
b. Variance inflation factors (VIFs)
c. Residual diagnostic plots
d. Adjusted $R^2$


**Question 4:** The definitions for AIC and BIC (or SBC) are: $$AIC = −2\ln(L) + 2p$$ $$BIC = −2\ln(L) + \ln\left(n\right)p$$
where $L$ is the log likelihood, $p$ is the number of parameters, $n$ is the number of observations (i.e., the sample size). Which one of the following statements is correct?

a. AIC penalizes complex models more than BIC
b. One can only use AIC as a model selection criterion in stepwise regression
c. For the same model, if the AIC is less than the BIC then it is an indication that you have found a good model
d. When BIC is used as the model selection criterion, usually fewer independent variables are included in the model compared with the model selected using AIC


**Question 5:** In simple linear regression, a confidence interval for the mean response is narrower for values of $X$ that are:

a. Closer to the mean of the the $X$'s
b. Further from the mean of the $X$'s


**Question 6:** When building a linear regression model of the form $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$ for a data set, the presence of multicollinearity in the data may result in _________ standard errors for the coefficient estimates than if the data came from an orthogonal design (i.e., when $X_1$ and $X_2$ are uncorrelated).

a. Smaller 
b. Larger


**Question 7:** What does the following residual versus fitted value plot suggest about a model between a single predictor $X$ and $Y$?

```{r, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="70%"}
set.seed(7051)
x <- runif(100, min = 0, max = 1)
y <- rnorm(100, mean = 1 + 2*x^2, sd = 0.1)
fit <- lm(y ~ x)
plot(fitted(fit), rstudent(fit), pch = 19, 
     xlab = "Fitted value", ylab = "Studentized residual",
     col = adjustcolor("darkblue", alpha.f = 0.5))
abline(h = 0, lty = 2, col = adjustcolor("darkred", alpha.f = 0.5))
```

a. Heteroscedasticity of the error term
b. A nonlinear between $X$ and $Y$
c. Non-normality of the error term
d. Satisfactory residuals


**Question 8:** When a linear regression model is being developed, adding additional variables to the model will

a. always decrease model SSE
b. always decrease model AIC
c. always increase model adjusted $R^2$
d. always decrease model MSE


**Question 9:** The ordinary residuals refer to

a. $\bar{Y} - \widehat{Y}$
b. $Y - \widehat{Y}$
c. $\bar{Y} - \bar{Y}$
d. $\widehat{Y} - Y$


**Question 10:** For a fitted simple linear regression model, which one of the following properties is **NOT** true?

a. The fitted regression line passes through the point $\left(\bar{X}, \bar{Y}\right)$
b. The residuals sum to zero: $\sum_{i=1}^n e_i = 0$
c. $\sum_{i=1}^n Y_i = \sum_{i=1}^n \widehat{Y}_i$
d. The residuals, $e_i$,. are always independent


**Question 11:** The diagonal elements of hat matrix, also referred to as the *hat values* or *leverage values*, measures the influence of observation $i$ on the regression line when removing observation $i$.

a. True 
b. False


**Question 12:** In a regression study, a 95% confidence interval for $\beta_1$ was given as: $\left(-5, 2\right)$. What does this confidence interval mean?

a. The interval $\left(-5, 2\right)$ contains the true $\beta_1$ with 95% probability
b. 95% of all possible $\beta_1$'s are in$\left(-5, 2\right)$
c. 95% of the interval $\left(-5, 2\right)$ contains the true $\beta_1$
d. If we were to repeat the experiment many times (i.e., repeatedly take a new sample of size $n$ and compute the same confidence interval), roughly 95% of the generated intervals would contain the true $\beta_1$


**Question 13:** In a regression study, a 95% confidence interval for $\beta_1$ was given as: $\left(-5, 2\right)$. Which of following is correct?

a. The 90% interval will be wider than the 95% interval
b. The 99% interval will be wider than the 95% interval


**Question 14:** In simple linear regression, the confidence interval for the mean response is always narrowest around $\bar{X}$

a. True 
b. False


**Question 15:** Point A in the far right is likely to be

```{r, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="70%"}
set.seed(7051)
x <- c(1:10/2, 15)
y <- rnorm(length(x), mean = 1 + 2*x, sd = 1)
plot(x, y, pch = 19, cex = 1.5, col = adjustcolor("darkred", alpha.f = 0.7), 
     las = 1, xlab = "X", ylab = "Y")
text(x[11], y[[11]], label = "A", pos = 1, cex = 1.5)
```


a. High influential point of the regression line
b. High leverage point


**Question 16:** Suppose you have built a multiple linear regression model according to $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \epsilon$. If you were to test $H_0: \beta_1 = \beta_3 = 0$ versus $H_1: \beta_1 \ne 0 \text{ or } \beta_3 \ne 0$ you would use

a. $t$-test
b. $F$-test based on an ANOVA
c. Partial $F$-test


**Question 17:** In simple linear regression, assume we used a $t$-test for $H_0: \beta_1 = 0 \text{ vs } H_1: \beta_1 \ne 0$, and we rejected the null hypothesis at the specified $\alpha$ level. We would conclude that:

a. The linear regression model is useful compared with $Y$
b. There is no linear relationship between $X$ and $Y$
c. The relationship between $X$ and $Y$ is quadratic
d. There is no relationship between $X$ and $Y$


**Question 18:** Which of the following cases lead to multicollinearity?

a. There are indicator variables being used as predictors
b. A predictor can be expressed as a linear combination of other predictors
c. The variances across all predictors are not the same
d. The predictors are not normally distributed and are positively skewed


**Question 19:** What does the last line of output "F-statistic....p-value: ..." indicate in the following multiple regression output?

```{r, echo=FALSE}
summary(lm(mpg ~ cyl + disp + hp + wt, data = mtcars))
```

a. All of the coefficients are significantly different from 0 at the $\alpha = 0.05$ level
b. None of the independent variables explains any of the variation in $Y$
c. At least one of the independent variables explains some of the variation in $Y$
d. The model explained 1.061e-10 of the variability in $Y$


**Question 20:** If one wishes to incorporate seasonal dummy variables for monthly data into a regression model, how many dummy variables should be in the model?

a. 12
b. 11
c. 10
d. 1


## Part II: short answer response

For the multiple linear regression model $Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \dots + \beta_p X_{p-1, i} + \epsilon_i$

**Question 21:** (2 pts.) Recall that the variance inflation factor for $X_j$ is defined as $VIF_j = \left(1 - R_j^2\right)^{-1}$. Explain in one sentence what $R_j^2$ is. Explain in one sentence why it is better to use VIFs than pairwise correlations when trying to detect the presence of multicollinearity in a regression data set.


**Question 22:** (2 pts.) For variable selection criteria based on residuals, among $PRESS$, $SSE$, $R^2$, $R_{adj}^2$, and $MSE$, which are "good" criteria for variable selection? Explain your reason in two sentences.


**Question 23:** (4 pts.) Please state one possible violation of standard model assumptions for each of the residual plots below (one sentence for each plot):

```{r example-diagnostics, echo=FALSE, fig.width=6, fig.asp=1, out.width="70%"}
# Simualated data sets
set.seed(101)
n <- 100
df1 <- tibble::tibble(
  x = runif(n, min = -5, max = 5),
  y = 1 + 2*x^2 + rnorm(n, sd = 10)
)
df2 <- tibble::tibble(
  x = runif(n, min = 1, max = 10),
  y = 1 + 4*x + rnorm(n, sd = 2*x)
)
df2 <- rbind(df2, data.frame(x = 2, y = 50))
df3 <- tibble::tibble(
  x = runif(n, min = 1, max = 10),
  y = 1 + 4*x + arima.sim(list(order = c(1,0,0), ar = 0.99), n = n, sd = 20)
)
df4 <- tibble::tibble(
  x = runif(n, min = 1, max = 10),
  y = 1 + 4*x + rlnorm(n, sd = 0.9)
)

# Fitted models
fit1 <- lm(y ~ x, data = df1)
fit2 <- lm(y ~ x, data = df2)
fit3 <- lm(y ~ x, data = df3)
fit4 <- lm(y ~ x, data = df4)

# Residuals
r1 <- residuals(fit1)
r2 <- rstandard(fit2)
r3 <- residuals(fit3)
r4 <- residuals(fit4)

# Residual plots
par(mfrow = c(2, 2))
plot(df1$x, r1, xlab = expression(X[1]), ylab = "Residual")
abline(h = 0, lty = "dotted", col = "red2")
plot(df2$x, r2, xlab = expression(X[1]), ylab = "Residual")
abline(h = 0, lty = "dotted", col = "red2")
plot(residuals(fit3), xlab = "Index", ylab = "Residual", type = "l")
points(residuals(fit3), col = adjustcolor("black", alpha.f = 0.2))
abline(h = 0, lty = "dotted", col = "red2")
qqnorm(r4, ylim = c(-5, 15))
qqline(r4, lty = "dotted", col = "red2")
```


**Question 24:** (4 pts.) Suppose you want to build a linear regression for response variable weight ( $Y$ ) using covariate height ( $X_1$ ) and gender ( $X_2 = 0$ for female and $X_2 = 1$ for male).

a. Suppose you want to allow the slope (of height) to be the same for different gender but different intercept, how would you build the linear regression model? Please specify the model in one line.

b. Suppose you want to allow both slope (of height) and intercept to be different for different gender, how would you build the linear regression model? Please specify the model in one line.

**Question 25:** (2 pts.) An engineer has stated: "Reduction of the number of candidate explanatory variables should always be done using the objective forward stepwise regression procedure." Discuss.

**Question 26:** (4 pts.) A junior investment analyst used a polynomial regression model of relatively high order in a research seminar on municipal bonds and obtained an $R^2$ of 0.991 in the regression of net interest yield of bond ( $Y$ ) on industrial diversity index of municipality ( $X$ ) for seven bond issues. A classmate, unimpressed, said: "You overfitted. Your curve follows the random effects in the data."

a. Comment on the classmate's criticism.

b. Might $R_{adj}^2$ be more appropriate than $R^2$ as a descriptive measure here?

**Question 27:** (4 pts.) A student who used a regression model that included indicator variables was upset when receiving only the following output on the multiple regression printout: XTRANSPOSE X SINGULAR. What does this mean and what is a likely source of the error?

**Question 28:** (4 pts.) In a regression study of factors affecting learning time for a certain task (measured in minutes), gender of learner was included as a predictor variable ($X_2$) that was coded $X_2 = 1$ if male and $X_2 = 0$ if female. It was found that the estimated coefficient of $X_2$ was $\widehat{\beta}_2 = 22.3$ with a standard error of 3.8. An observer questioned whether the coding scheme for gender is fair because it results in a positive coefficient, leading to longer learning times for males than females. Comment.

**Question 29:** (2 pts.) A student stated: "Adding predictor variables to a regression model can never reduce $R^2$, so we should include all available predictor variables in the model." Comment.

**Question 30:** (2 pts.) The members of a health spa pay annual membership dues of \$300 plus a charge of \$2 for each visit to the spa. Let $Y$ denote the dollar cost for the year for a member and $X$ the number of visits by the member during the year. Express the relation between $X$ and $Y$ mathematically. Is it a functional relation or a statistical relation?

**Question 31:** (2 pts.) Evaluate the following statement: "For the least squares method to be fully valid, it is required that the distribution of $Y$ be normal."

**Question 32:** (2 pts.) A member of a student team playing an interactive marketing game received the following computer output when studying the relation between advertising expenditures ( $X$ ) and sales ( $Y$ ) for one of the team's products:

* Estimated regression equation: $Y = 350.70 - 0.18X$
* Two-sided *p*-value for estimated slope: $0.91$

The student stated: "The message I get here is that the more we spend on advertising this
product, the fewer units we sell!" Comment.

**Question 33:** (2 pts.) What is a residual? Why are residuals important in regression analysis?

**Question 34:** (4 pts.) The following regression model is being considered in a water resources study: $$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{1i}X_{2i} + \beta_4 \sqrt{X_{3i}} + \epsilon_i$$ State the reduced models for testing whether or not: 

a. $\beta_3 = \beta_4 = 0$
b. $\beta_3 = 0$
c. $\beta_1  = \beta_2 = 5$
d. $\beta_4 = 7$

**Bonus:** (5 pts.) An analyst wanted to fit the regression model $Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \dots + \beta_p X_{p-1, i} + \epsilon_i$ by the method of least squares when it is known that $\beta_2 = 4$. How can the analyst obtain the desired fit using standard statistical software (e.g., R, Python, or SAS)? No need to run any code, just describe in general how you could accomplish this.